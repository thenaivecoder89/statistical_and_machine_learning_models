{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37ee594",
   "metadata": {},
   "source": [
    "# Analysis of MF Data from AMFI and Portfolio Development\n",
    "\n",
    "This notebook aims to analyze all listed MFs under AMFI (Association of Mutual Funds of India) to provide insights on volatility and historical performance of each fund and leverage the same to suggest recommendations for investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c167d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import date, datetime\n",
    "from kiteconnect import KiteConnect\n",
    "\n",
    "# Import rapids specific libraries\n",
    "def load_rapids_env():\n",
    "    import cudf\n",
    "    import cupy as cp\n",
    "    import connectorx as cx\n",
    "    return cudf, cp, cx\n",
    "\n",
    "# Initialize environment\n",
    "load_dotenv()\n",
    "amfi_data_batchA = os.getenv('amfi_data_batchA')\n",
    "amfi_data_batchB = os.getenv('amfi_data_batchB')\n",
    "railway_db_url = os.getenv('railway_db_url')\n",
    "engine = create_engine(railway_db_url, connect_args={'options': '-c search_path=\"FINANCIAL_ANALYSIS\"'})\n",
    "\n",
    "# Select execution option\n",
    "option = input('Select program to run: 1-Data_Load, 2-Funds_Analysis_Master_Data_Development, 3-Funds_Analysis: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d50fd94",
   "metadata": {},
   "source": [
    "## Data load program - to be executed only once in the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "dfbf7da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected option 2. Proceeding to execute funds analysis program.\n"
     ]
    }
   ],
   "source": [
    "# Data load program\n",
    "if option == '1':\n",
    "    try:\n",
    "        # Load data from batch A into dataframe and correct date format\n",
    "        df_batchA = pd.read_csv(amfi_data_batchA)\n",
    "        df_batchA['date'] = pd.to_datetime(df_batchA['date'], infer_datetime_format= True, errors = 'coerce')\n",
    "        print(f'Data for top 5 rows from batch A: \\n{df_batchA.head(5)}')\n",
    "        errors_batchA = df_batchA['date'].isna()\n",
    "        i_batchA = [rows for rows, val in enumerate(errors_batchA) if val == True]\n",
    "        print(f'Number of records with dates in string and not updated by Pandas in batch A: {len(i_batchA)}')\n",
    "        df_batchA = df_batchA.dropna(subset=['date'])\n",
    "        print(f'Number of records in batch A: {len(df_batchA['SNo.'])}')\n",
    "\n",
    "        # Load data from batch B into dataframe and correct date format\n",
    "        df_batchB = pd.read_csv(amfi_data_batchB)\n",
    "        df_batchB['date'] = pd.to_datetime(df_batchB['date'], infer_datetime_format= True, errors='coerce')\n",
    "        print(f'Data for top 5 rows from batch B: \\n{df_batchB.head(5)}')\n",
    "        errors_batchB = df_batchB['date'].isna()\n",
    "        i_batchB = [rows for rows, val in enumerate(errors_batchB) if val == True]\n",
    "        print(f'Number of records with dates in string and not updated by Pandas in batch B: {len(i_batchB)}')\n",
    "        df_batchB = df_batchB.dropna(subset=['date'])\n",
    "        print(f'Number of records in batch B: {len(df_batchB['SNo.'])}')\n",
    "\n",
    "        # Combine batch A and B data\n",
    "        df_combined = pd.concat([df_batchA, df_batchB], ignore_index=True)\n",
    "        df_combined = df_combined.rename(columns={'date': 'trx_date'})\n",
    "        df_combined = df_combined.rename(columns={'SNo.': 's_no'})\n",
    "        print(f'Data for top 5 rows from consolidated dataframe: \\n{df_combined.head(5)}')\n",
    "        print(f'Number of records in consolidated data: {len(df_combined['s_no'])}')\n",
    "\n",
    "        # Populate data into database\n",
    "        with engine.connect() as database_connection:\n",
    "            for records_start in range(0, len(df_combined), 1000000):\n",
    "                records_end = records_start + 1000000\n",
    "                df_chunks = df_combined.iloc[records_start:records_end]\n",
    "                df_chunks.to_sql(\n",
    "                    'amfi_database',\n",
    "                    con=database_connection,\n",
    "                    schema='FINANCIAL_ANALYSIS',\n",
    "                    if_exists='append',\n",
    "                    index=False,\n",
    "                    method='multi'\n",
    "                    )\n",
    "                database_connection.commit()\n",
    "                print(f'{len(df_chunks)} Committed.')\n",
    "\n",
    "        query = text('select * from amfi_database;')\n",
    "        with engine.connect() as database_connection:\n",
    "            df = pd.read_sql(sql=query, con=database_connection, index_col='trx_id')\n",
    "\n",
    "        processed_records = len(df['s_no'])\n",
    "        print(f'Successfully entered {processed_records} into database.')\n",
    "    except Exception as e:\n",
    "        print(f'Error Encountered During Data Load: {e}')\n",
    "else:\n",
    "    print(f'Selected option 2. Proceeding to execute funds analysis program.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7d786",
   "metadata": {},
   "source": [
    "## Funds analysis program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e36a2",
   "metadata": {},
   "source": [
    "### 1. Initial data load and transformation using RAPIDS to generate final dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "387c89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected option 3, running the data analysis program.\n"
     ]
    }
   ],
   "source": [
    "# Funds analysis program - data pull and load\n",
    "if option == '2':\n",
    "    try:\n",
    "        # Initialize environment and obtain necessary keys\n",
    "        cudf, cp, cx = load_rapids_env()\n",
    "        kite_api = os.getenv('kite_connect_api')\n",
    "        rapids_data_to_path = os.getenv('rapids_data_to_path')\n",
    "        final_dataset = os.getenv('final_dataset')\n",
    "        parquet_data = os.getenv('parquet_data')\n",
    "        \n",
    "        # Pull data into arrow table using connectorx and convert to GPU enabled dataframe.\n",
    "        query = 'select * from \"FINANCIAL_ANALYSIS\".amfi_database where trading_symbol_reinvestment is not null and trading_symbol_growth is not null;'\n",
    "        nav_historical_arrow_tbl = cx.read_sql(\n",
    "            conn=railway_db_url, \n",
    "            query=query, \n",
    "            return_type='arrow',\n",
    "            partition_on='trx_id',\n",
    "            partition_range=(1, 20000000),\n",
    "            partition_num=7\n",
    "            )\n",
    "        df_nav_historical_data = cudf.DataFrame.from_arrow(nav_historical_arrow_tbl)\n",
    "        df_nav_historical_data = df_nav_historical_data.sort_values(by='s_no')\n",
    "        print(f'MFs with available trading symbols:{len(df_nav_historical_data)}')\n",
    "        df_nav_historical_data_20_year = df_nav_historical_data[\n",
    "                df_nav_historical_data['trx_date'].dt.year>=2005\n",
    "            ]\n",
    "        print(f'20 year historical records: {len(df_nav_historical_data_20_year)}')\n",
    "\n",
    "        # Pull data from kite on MF details (not available in AMFI)\n",
    "        kc = KiteConnect(api_key=kite_api)\n",
    "        kc_mf_instruments = kc.mf_instruments()\n",
    "\n",
    "        # Mutating the kite date columns to GPU friendly datetime type.\n",
    "        for rec in kc_mf_instruments:\n",
    "            for items, value in rec.items():\n",
    "                if isinstance(value, (date, datetime)):\n",
    "                    rec[items] = value.isoformat()\n",
    "\n",
    "        df_kc_mf_data = cudf.DataFrame(kc_mf_instruments)\n",
    "        df_kc_mf_data.index.name = 'SNo.'\n",
    "        df_kc_mf_data['last_price_date'] = df_kc_mf_data['last_price_date'].astype('str')\n",
    "        df_kc_mf_data['last_price_date'] = cudf.to_datetime(df_kc_mf_data['last_price_date'])\n",
    "        print(f'Total records pulled from kite on MF instruments: {len(df_kc_mf_data)}')\n",
    "\n",
    "        # Merging AMFI and KTIE data into final master data for future analysis.\n",
    "        df_amfi_kc_merged = cudf.merge(\n",
    "            df_nav_historical_data_20_year, \n",
    "            df_kc_mf_data, \n",
    "            left_on='trading_symbol_growth', \n",
    "            right_on='tradingsymbol', \n",
    "            how='left'\n",
    "            )\n",
    "        \n",
    "        # Pushing final data into CSV.\n",
    "        df_amfi_kc_merged.to_csv(f'{rapids_data_to_path}/00.AMFI_KITE_FINAL_MF_20_YEAR_DATA.csv')\n",
    "        print(f'Data merge complete. {len(df_amfi_kc_merged)} records in final dataset.')\n",
    "\n",
    "        # Load and cleanse data\n",
    "        df_final_dataset = cudf.read_csv(final_dataset)\n",
    "        df_final_dataset = df_final_dataset.dropna(subset='tradingsymbol') # Removing all rows with missing Kite data\n",
    "        missing_records = df_final_dataset['tradingsymbol'].isna().sum()\n",
    "        df_final_dataset['trx_date'] = cudf.to_datetime(df_final_dataset['trx_date']) # Converting all dates to datetime format\n",
    "        print(f'Number of records with missing Zerodha data: {missing_records}')\n",
    "        print(f'Total remaining records in final cleansed dataset: {len(df_final_dataset)}')\n",
    "\n",
    "        # Create unique datasets based on fund names\n",
    "        df_final_dataset.to_parquet(f'{rapids_data_to_path}/parquet_data/', partition_cols=['scheme_name'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error Encountered During Funds Analysis Master Data Development: {e}')\n",
    "\n",
    "elif option == '1':\n",
    "    print('Selected option 1, running the data load program.')\n",
    "\n",
    "else:\n",
    "    print(f'Selected option 3, running the data analysis program.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58639ad8",
   "metadata": {},
   "source": [
    "### 2. Feature engineering on final dataset to enable fund volatility analysis computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "be4f672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for ['360 ONE Dynamic Bond Fund Direct Plan Monthly Dividend', '360 ONE Dynamic Bond Fund Direct Plan Quarterly Dividend']: \n",
      "                                            scheme_name            fund_name  \\\n",
      "2987  360 ONE Dynamic Bond Fund Direct Plan Quarterl...  360 ONE Mutual Fund   \n",
      "3002  360 ONE Dynamic Bond Fund Direct Plan Quarterl...  360 ONE Mutual Fund   \n",
      "2983  360 ONE Dynamic Bond Fund Direct Plan Quarterl...  360 ONE Mutual Fund   \n",
      "2991  360 ONE Dynamic Bond Fund Direct Plan Quarterl...  360 ONE Mutual Fund   \n",
      "2992  360 ONE Dynamic Bond Fund Direct Plan Quarterl...  360 ONE Mutual Fund   \n",
      "...                                                 ...                  ...   \n",
      "2975  360 ONE Dynamic Bond Fund Direct Plan Monthly ...  360 ONE Mutual Fund   \n",
      "2977  360 ONE Dynamic Bond Fund Direct Plan Monthly ...  360 ONE Mutual Fund   \n",
      "2978  360 ONE Dynamic Bond Fund Direct Plan Monthly ...  360 ONE Mutual Fund   \n",
      "2979  360 ONE Dynamic Bond Fund Direct Plan Monthly ...  360 ONE Mutual Fund   \n",
      "2980  360 ONE Dynamic Bond Fund Direct Plan Monthly ...  360 ONE Mutual Fund   \n",
      "\n",
      "     tradingsymbol scheme_type settlement_type   trx_date      nav  \\\n",
      "2987  INF579M01290        Debt              T1 2025-10-31  22.5395   \n",
      "3002  INF579M01290        Debt              T1 2025-10-30  22.5412   \n",
      "2983  INF579M01290        Debt              T1 2025-10-29  22.5712   \n",
      "2991  INF579M01290        Debt              T1 2025-10-28  22.5371   \n",
      "2992  INF579M01290        Debt              T1 2025-10-27  22.5623   \n",
      "...            ...         ...             ...        ...      ...   \n",
      "2975  INF579M01282        Debt              T1 2013-07-02   9.9727   \n",
      "2977  INF579M01282        Debt              T1 2013-06-28  10.0230   \n",
      "2978  INF579M01282        Debt              T1 2013-06-27   9.9883   \n",
      "2979  INF579M01282        Debt              T1 2013-06-26   9.9750   \n",
      "2980  INF579M01282        Debt              T1 2013-06-25  10.0042   \n",
      "\n",
      "     nav_returns_%age  \n",
      "2987             <NA>  \n",
      "3002           0.0075  \n",
      "2983           0.1331  \n",
      "2991          -0.1511  \n",
      "2992           0.1118  \n",
      "...               ...  \n",
      "2975          -0.2909  \n",
      "2977           0.5044  \n",
      "2978          -0.3462  \n",
      "2979          -0.1332  \n",
      "2980           0.2927  \n",
      "\n",
      "[4933 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Funds analysis program - feature engineering\n",
    "if option == '3':\n",
    "    try:\n",
    "        # Initilaize environment\n",
    "        cudf, cd, cx = load_rapids_env()\n",
    "        load_dotenv()\n",
    "        parquet_data = os.getenv('parquet_data')\n",
    "        rapids_data_to_path = os.getenv('rapids_data_to_path')\n",
    "\n",
    "        # Load parquet data and generate %NAV change (proxy for return calculations)\n",
    "        scheme_name = ['360 ONE Dynamic Bond Fund Direct Plan Monthly Dividend', '360 ONE Dynamic Bond Fund Direct Plan Quarterly Dividend']\n",
    "        df_parquet_dataset = cudf.read_parquet(parquet_data, filters=[('scheme_name', 'in', scheme_name)])\n",
    "        df_parquet_dataset = df_parquet_dataset.sort_values(by=['scheme_name', 'trx_date'], ascending=False)\n",
    "        df_parquet_dataset['nav_returns_%age'] = ((df_parquet_dataset['nav'].pct_change())*100).astype('float64').round(4)\n",
    "        df_parquet_dataset = df_parquet_dataset[['scheme_name', 'fund_name', 'tradingsymbol', 'scheme_type', 'settlement_type', 'trx_date', 'nav', 'nav_returns_%age']]\n",
    "        print(f'Data for {scheme_name}: \\n{df_parquet_dataset}')\n",
    "        df_parquet_dataset.to_csv(f'{rapids_data_to_path}/01.nav_returns_dataset.csv')\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error Encountered During Fund Analysis: {e}')\n",
    "\n",
    "else:\n",
    "    print(f'Invalid option {option} selected. Please enter either 1, 2 or 3 as input.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
