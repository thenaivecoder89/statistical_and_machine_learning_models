{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9564a794",
   "metadata": {},
   "source": [
    "## **Section 1: Introduction and Summary**\n",
    "\n",
    "This notebook intends to analyze, compare, and provide insights on the performance of two fundamentally different approaches to **portfolio risk modelling and risk-adjusted portfolio construction**:\n",
    "\n",
    "- One **machine-learning–based risk modelling framework**, and  \n",
    "- One **Monte Carlo simulation–based risk modelling framework**.\n",
    "\n",
    "The comparison is conducted across the following dimensions:\n",
    "\n",
    "**1. Risk Capture and Sensitivity:**  \n",
    "How effectively each approach captures volatility dynamics, downside risk, tail behavior, and regime-dependent uncertainty embedded in historical return series.\n",
    "\n",
    "**2. Stability of Risk Estimates:**  \n",
    "The robustness and consistency of estimated risk inputs—particularly volatility and covariance—under evolving market conditions and across rebalancing windows.\n",
    "\n",
    "**3. Portfolio Construction and Rebalancing Outcomes:**  \n",
    "How differences in risk estimation propagate into portfolio weights, diversification properties, and realized risk-adjusted performance over time.\n",
    "\n",
    "Based on the findings above, this notebook aims to determine **Model Applicability**—that is, which framework is better suited for **risk estimation, portfolio optimization support, and stress-aware decision making**, rather than return forecasting.\n",
    "\n",
    "Additionally, the notebook explains the underlying mechanics—both mathematically and intuitively—of each approach, enabling a clear understanding of how uncertainty is modelled and translated into portfolio decisions.\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset Description**\n",
    "\n",
    "The analysis uses **daily closing price data** spanning approximately **2001–2026**, covering multiple market regimes including the Global Financial Crisis (2008), the COVID-19 shock (2020), and the post-pandemic monetary tightening cycle.\n",
    "\n",
    "The asset universe is intentionally restricted to three broad instruments to emphasize **diversification, correlation structure, and regime behavior**, rather than security-selection alpha:\n",
    "\n",
    "1. **NIFTY 50 Index** – proxy for broad Indian equity market exposure (sourced from Zerodha using Kite API)\n",
    "2. **NIFTY Bank Index** – proxy for cyclical, leverage-sensitive financial equities (sourced from Zerodha using Kite API)\n",
    "3. **Gold** – proxy for defensive and inflation-hedging behavior (sourced from Yahoo Finance - COMEX Gold Futures - CMEGroup - GC=F)\n",
    "\n",
    "Daily prices are transformed into **logarithmic returns**, which form the foundational input for all subsequent modelling stages. Log returns ensure time-additivity, scale consistency, and comparability across assets with heterogeneous price levels.\n",
    "\n",
    "---\n",
    "\n",
    "**Analytical Objective**\n",
    "\n",
    "Unlike return-forecasting exercises, this notebook focuses explicitly on **risk estimation and portfolio behavior**.\n",
    "\n",
    "The primary objectives are to:\n",
    "\n",
    "- Estimate asset-level and portfolio-level risk under different modelling paradigms  \n",
    "- Construct portfolios using a **fixed, risk-adjusted optimization rule**  \n",
    "- Evaluate how alternative risk models influence:\n",
    "  - Estimated volatility and covariance structure  \n",
    "  - Optimal portfolio weights  \n",
    "  - Realized post-rebalancing performance  \n",
    "\n",
    "Importantly, the notebook does **not** attempt to predict future prices or returns directly. Instead, it evaluates how different risk-modelling philosophies interpret uncertainty and how that uncertainty affects portfolio outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "**Modelling Approach**\n",
    "\n",
    "Two distinct and complementary modelling frameworks are implemented.\n",
    "\n",
    "*Monte Carlo Risk Modelling*\n",
    "\n",
    "The Monte Carlo framework represents a **distribution-based, probabilistic approach** to risk estimation. Using historical return data, the model:\n",
    "\n",
    "- Estimates expected returns and covariance structure  \n",
    "- Simulates a large number of potential future return paths  \n",
    "- Aggregates simulated outcomes to infer portfolio-level risk measures  \n",
    "\n",
    "This approach explicitly models **distributional uncertainty** and is particularly well-suited for understanding tail risk and scenario dispersion.\n",
    "\n",
    "*Machine Learning Risk Modelling*\n",
    "\n",
    "The machine learning framework treats **risk as a conditional, learnable function of historical return dynamics**. Rather than assuming a fixed parametric distribution, the model infers patterns such as:\n",
    "\n",
    "- Volatility clustering  \n",
    "- Non-linear dependencies  \n",
    "- Regime-dependent behavior  \n",
    "\n",
    "Engineered features encode memory, dispersion, and drawdown effects observed in financial time series. Model outputs are used to generate **forward-looking volatility estimates**, which feed directly into portfolio construction.\n",
    "\n",
    "---\n",
    "\n",
    "**Portfolio Construction and Evaluation**\n",
    "\n",
    "Portfolio construction is governed by a **single, fixed decision rule** applied consistently across both modelling approaches.\n",
    "\n",
    "- **Risk-adjusted return metric:** Sharpe ratio  \n",
    "- **Optimization objective:** Maximize Sharpe ratio  \n",
    "- **Constraints:**  \n",
    "  - Long-only  \n",
    "  - Fully invested  \n",
    "\n",
    "By comparing **pre- and post-rebalancing portfolios under each risk model and portfolio construction rule**, the notebook highlights how **model choice alone can materially alter portfolio behavior—even when using the same underlying assets**.\n",
    "\n",
    "Evaluation focuses on:\n",
    "\n",
    "- Portfolio volatility  \n",
    "- **Risk-adjusted return metrics: Sharpe**  \n",
    "- Drawdown and tail-risk characteristics  \n",
    "- Stability of portfolio weights and turnover\n",
    "\n",
    "---\n",
    "\n",
    "**Project Methodology**\n",
    "\n",
    "The empirical analysis follows a structured, walk-forward research design to ensure methodological rigor and avoid look-ahead bias.\n",
    "\n",
    "*Step 1: Data Collection and Cleansing*\n",
    "\n",
    "Three independent price series are sourced:\n",
    "\n",
    "- NIFTY 50 prices  \n",
    "- NIFTY Bank prices  \n",
    "- Gold price proxy  \n",
    "\n",
    "All datasets are:\n",
    "\n",
    "- Cleaned for missing observations  \n",
    "- Aligned to a common date range  \n",
    "- Prepared to ensure consistency across assets\n",
    "\n",
    "*Step 2: Computation of Daily Log Returns*\n",
    "\n",
    "For each asset:\n",
    "\n",
    "- Daily log returns are computed  \n",
    "- This return series constitutes the **only shared input across all models**  \n",
    "\n",
    "All downstream modelling—Monte Carlo and ML—is built exclusively on this return representation.\n",
    "\n",
    "*Step 3: Feature Engineering (Asset-Specific)*\n",
    "\n",
    "For each asset independently, the following features are engineered:\n",
    "\n",
    "- Rolling volatility estimates (20-day, 60-day)  \n",
    "- Lagged return terms  \n",
    "- Rolling mean returns  \n",
    "- Drawdown-based measures  \n",
    "\n",
    "These features are **used exclusively by the machine learning framework** and are not inputs to the Monte Carlo simulation.\n",
    "\n",
    "*Step 4: Portfolio Decision Rule Definition*\n",
    "\n",
    "Prior to any modelling, the portfolio construction rule is defined **once and fixed throughout the study**.\n",
    "\n",
    "- **Objective:** Maximize Sharpe ratio  \n",
    "- **Constraints:**  \n",
    "  - Long-only (only positive positions held in assets - no short positions considered)\n",
    "  - Fully invested (portfolio weight = 100%)\n",
    "\n",
    "This ensures that differences in outcomes arise solely from **risk estimation**, not from changing optimization logic.\n",
    "\n",
    "*Step 5: Walk-Forward Portfolio Construction*\n",
    "\n",
    "This step constitutes the core of the project and is repeated at each monthly rebalancing date.\n",
    "\n",
    "*Step 5A — Monte Carlo Risk Estimation*\n",
    "\n",
    "At rebalancing date $T$:\n",
    "\n",
    "- Use only historical returns available up to $T$ (e.g., trailing 3 years)  \n",
    "- Estimate:\n",
    "  - Mean returns  \n",
    "  - Covariance matrix (volatility and correlations)  \n",
    "- Run Monte Carlo simulations to generate future return paths  \n",
    "- Compute risk inputs for portfolio optimization  \n",
    "- Pass these inputs into the Sharpe optimizer  \n",
    "- Obtain portfolio weights:\n",
    "\n",
    "$$\n",
    "(x^{MC}, y^{MC}, z^{MC})\n",
    "$$\n",
    "\n",
    "*Step 5B — Machine Learning Risk Estimation*\n",
    "\n",
    "At the same rebalancing date $T$:\n",
    "\n",
    "- Train ML models (per asset) using only historical features  \n",
    "- Predict next-period volatility for each asset  \n",
    "- Combine predicted volatilities with rolling correlation estimates  \n",
    "- Construct a forecasted covariance matrix  \n",
    "- Pass these inputs into the same Sharpe optimizer  \n",
    "- Obtain portfolio weights:\n",
    "\n",
    "$$\n",
    "(x^{ML}, y^{ML}, z^{ML})\n",
    "$$\n",
    "\n",
    "*Step 6: Out-of-Sample Holding Period*\n",
    "\n",
    "- Both portfolios are held for the subsequent month  \n",
    "- Realized portfolio returns are recorded  \n",
    "\n",
    "*Step 7: Iteration Through Time*\n",
    "\n",
    "Steps 5 and 6 are repeated across the full sample period, generating:\n",
    "\n",
    "- A Monte Carlo–based portfolio return series  \n",
    "- A Machine Learning–based portfolio return series  \n",
    "\n",
    "*Step 8: Comparative Evaluation*\n",
    "\n",
    "The two portfolios are compared across:\n",
    "\n",
    "- Sharpe ratio  \n",
    "- Maximum drawdown  \n",
    "- Conditional Value-at-Risk (CVaR)  \n",
    "- Volatility  \n",
    "- Turnover  \n",
    "- Behavior during market stress periods\n",
    "\n",
    "---\n",
    "\n",
    "**Disclaimer**\n",
    "\n",
    "This notebook is intended solely for **methodological comparison and analytical insight**. The portfolios constructed are illustrative and do not constitute investment advice. Both modelling frameworks rely on historical information and implicitly assume that past statistical properties are informative of future risk. In practice, financial markets are subject to **structural breaks, policy interventions, and exogenous shocks** that may invalidate these assumptions. Accordingly, results should be interpreted as **indicative rather than definitive**, and any real-world deployment would require additional validation, stress testing, and governance controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ddd52a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "\n",
    "import os, cudf, cupy, time\n",
    "from dotenv import load_dotenv\n",
    "from scipy.stats import kurtosis, skew, jarque_bera, anderson\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import matplotlib.pyplot as plt, matplotlib.dates as mdates, seaborn as sns, numpy as np, pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb966177",
   "metadata": {},
   "source": [
    "## **Section 2: Data Load and Visualization**\n",
    "\n",
    "Since the raw data has already been pre-processed using a separate ETL pipeline (please refer to GitHub repo: https://github.com/thenaivecoder89/statistical_and_machine_learning_models/tree/main/data_sourcing filename: ZERODHA_YFINANCE_ETL_PROJECT_3.py for the full ETL codebase), this section will focus more on loading the pre-processed data and visualizing the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102beb19",
   "metadata": {},
   "source": [
    "### 2.1. Data Load and Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c99812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record program start time\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Initialize environment\n",
    "load_dotenv()\n",
    "kite_nifty50_daily_historical_data = os.getenv('kite_nifty50_daily_historical_data')\n",
    "kite_niftybank_daily_historical_data = os.getenv('kite_niftybank_daily_historical_data')\n",
    "gold_daily_historical_data = os.getenv('gold_daily_historical_data')\n",
    "xgb_master_data = os.getenv('xgb_master_data')\n",
    "\n",
    "# Load Data\n",
    "nifty50_cudf = cudf.read_csv(kite_nifty50_daily_historical_data)\n",
    "niftybank_cudf = cudf.read_csv(kite_niftybank_daily_historical_data)\n",
    "goldfutures_cudf = cudf.read_csv(gold_daily_historical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4bbe7",
   "metadata": {},
   "source": [
    "### 2.2. Exploratory Data Analysis (EDA) and Statistical Tests\n",
    "\n",
    "This section focuses on conducting EDA and statistical analysis on each of the 3 datasets:\n",
    " - NIFTY 50\n",
    " - NIFTY Bank\n",
    " - Gold Futures (considered as a proxy for Gold spot)\n",
    " \n",
    "To generate critical insights on the project way-forward across the following areas:\n",
    " - Monte-Carlo: Gaussian v/s Non-Gaussian (accounting for kurtosis, skewness and volatility clustering)\n",
    " - XGBoost: Feature engineering focus\n",
    " - Risk Metrics: Emphasis on Value at Risk (VaR) or Conditional Value at Risk (CVaR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ff4b8",
   "metadata": {},
   "source": [
    "#### 2.2.1. EDA and Tests on NIFTY50 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ed9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View summary statistics\n",
    "print('SUMMARY STATISTICS')\n",
    "print(f'NIFTY50 dataset - Top 5 and Bottom 5 rows:\\n{nifty50_cudf.head()}\\n{nifty50_cudf.tail()}')\n",
    "print(f'NIFTY50 summary statistics:\\n{nifty50_cudf.describe()}')\n",
    "print('========================================================')\n",
    "print('EDA')\n",
    "# Plot Data\n",
    "fig, ax = plt.subplots(figsize=(40, 15), dpi=100)\n",
    "nifty50_cudf['date'] = cudf.to_datetime(nifty50_cudf['date'])\n",
    "ax.plot(nifty50_cudf['date'], nifty50_cudf['daily_log_closing_value'], '--', color='red', marker='o')\n",
    "ax.set_title('NIFTY 50 Data Plot - Daily Log Returns', fontsize=30)\n",
    "ax.set_xlabel('Time', fontsize=20)\n",
    "ax.set_ylabel('Daily Log Returns', fontsize=20)\n",
    "ax.set_xlim(nifty50_cudf['date'].min(), nifty50_cudf['date'].max())\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "fig.tight_layout()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "fig_kde, ax_kde = plt.subplots(figsize=(40, 15), dpi=100)\n",
    "sns.kdeplot(nifty50_cudf['daily_log_closing_value'].to_numpy(), bw_adjust=1, bw_method='silverman', color='red', fill=True, ax=ax_kde)\n",
    "ax_kde.set_title('NIFTY 50 KDE Plot - Daily Log Returns', fontsize=30)\n",
    "ax_kde.set_xlabel('Daily Log Returns', fontsize=20)\n",
    "ax_kde.set_ylabel('Density', fontsize=20)\n",
    "fig_kde.tight_layout()\n",
    "plt.show()\n",
    "print('========================================================')\n",
    "print('STATISTICAL TESTS')\n",
    "# Tests\n",
    "print(f\"\"\"Following statistical tests are being conducted on the data to better understand the distribution guide our way-forward:\n",
    "      1. Kurtosis - to test for tail thickness\n",
    "      2. Skewness - to test for tail assymetry\n",
    "      3. Normality Rejection - Jarque–Bera (moment-based) and Anderson-Darling (tail-sensitive) - to support rejection of Gaussian MC\n",
    "      4. Volatility Clustering - ACF and Ljung-Box on squared returns - to support development of features for XGBoost model\n",
    "\"\"\")\n",
    "# 1. Kurtosis Test\n",
    "print('1. Kurtosis Test')\n",
    "ret_log = nifty50_cudf['daily_log_closing_value'].to_numpy()\n",
    "nifty50_kurt = kurtosis(\n",
    "    ret_log,\n",
    "    fisher=True, # capturing excess kurtosis = pearson kurtosis - 3\n",
    "    bias=False # apply bias correction on estimator\n",
    ")\n",
    "if nifty50_kurt == 0:\n",
    "    print(f'Kurtosis = {nifty50_kurt:.2f}. Outcome - Normal: Tail behavior is consistent with normality')\n",
    "elif nifty50_kurt > 0:\n",
    "    print(f\"\"\"\n",
    "Kurtosis = {nifty50_kurt:.2f}.\n",
    "Outcome:\n",
    "    Distribution is Leptokurtic (fat tails) - meaning there is a higher probability of extreme outcomes and volatility clustering is likely present. Thus, our project will adopt the below approach:\n",
    "    1. Monte-Carlo: During implementation, we will incorporate fat-tailed innovations (e.g., Student-t) to preserve tail behavior.\n",
    "    2. XGBoost: No winsorization of extremes. Outliers shall be retained as-is as they represent genuine market behavior rather than pure noise.\n",
    "    3. Risk Metrics: Conditional Value at Risk (CVaR) - over VaR - will be the preferred metric of choice to ensure sensitivity to tail risk.\n",
    "\"\"\")\n",
    "elif nifty50_kurt < 0:\n",
    "    print(f\"\"\"\n",
    "Kurtosis = {nifty50_kurt:.2f}.\n",
    "Outcome:\n",
    "    Distribution is Platykurtic (thin tails) - meaning there are fewer extreme events relative to a normal distribution. This suggests relatively stable return dynamics with reduced tail risk. Hence, our project will adopt the below approach:\n",
    "    1. Monte-Carlo: Parametric simulations with Gaussian or mildly fat-tailed innovations will be considered.\n",
    "    2. XGBoost: Limited winsorization or other outlier elimination methods shall be evaluated - depending upon the model's out-of-sample performance.\n",
    "    3. Risk Metrics: While CVaR will remain available for completeness, standard deviation and Sharpe-based metrics will receive greater emphasis given the reduced tail risk profile. \n",
    "\"\"\")\n",
    "x = ret_log[np.isfinite(ret_log)]\n",
    "nifty50_kurt_check = kurtosis(x, fisher=True, bias=False)\n",
    "print('Kurtosis sanity check:')\n",
    "if abs(nifty50_kurt - nifty50_kurt_check) < 1e-6:\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Fail')\n",
    "# 2. Skewness Test\n",
    "print('\\n2. Skewness Test')\n",
    "nifty50_skew = skew(\n",
    "    ret_log,\n",
    "    bias=False # apply bias correction on estimator\n",
    ")\n",
    "if nifty50_skew > 0:\n",
    "    print(f\"\"\"\n",
    "Skewness = {nifty50_skew:.2f}.\n",
    "Outcome:\n",
    "    Distribution is positively skewed (right-skewed) – indicating a heavier right tail and a higher frequency of large positive return realizations.\n",
    "    This suggests upside-dominated asymmetry, although tail risk is material since kurtosis is elevated (12.40). Accordingly, the project will adopt the following approach:\n",
    "    1. Monte-Carlo: Symmetric or mildly asymmetric fat-tailed innovations may be considered, subject to validation against empirical tails.\n",
    "    2. XGBoost: Retain positive return extremes, as they may capture convex payoff structures or regime-driven upside events.\n",
    "    3. Risk Metrics: CVaR will remain available for completeness, while performance evaluation may also consider upside-sensitive metrics where appropriate.\n",
    "\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"\n",
    "Skewness = {nifty50_skew:.2f}.\n",
    "Outcome:\n",
    "    Distribution is negatively skewed (left-skewed) – indicating a heavier left tail and a higher frequency of large negative returns relative to positive extremes.\n",
    "    This highlights pronounced downside asymmetry and elevated crash risk. Accordingly, the project will adopt the following approach:\n",
    "    1. Monte-Carlo: Consider asymmetric or skew-aware innovations (e.g., skew-t), or focus explicitly on left-tail behavior in simulations.\n",
    "    2. XGBoost: Preserve negative return extremes without winsorization, and prioritize downside-sensitive features (e.g., downside volatility, drawdown-related measures).\n",
    "    3. Risk Metrics: Emphasize left-tail CVaR as the primary risk metric to ensure sensitivity to downside tail risk.\n",
    "\"\"\")\n",
    "nifty50_skew_check = skew(x, bias=False)\n",
    "print('Skewness sanity check:')\n",
    "if abs(nifty50_skew - nifty50_skew_check) < 1e-6:\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Fail')\n",
    "# 3. Normality Rejection Tests (Jarque-Bera and Anderson-Darling)\n",
    "print('\\n3. Normality Rejection Tests (Jarque-Bera and Anderson-Darling)')\n",
    "print('3.1. Jarque-Bera Test: H0: Returns are normally distributed')\n",
    "nifty50_jb_stat, nifty50_jb_p = jarque_bera(x)\n",
    "alpha = 0.05\n",
    "print(f'Jarque-Bera Test Results: JB-Statistic = {nifty50_jb_stat:.2f} and P-Value = {nifty50_jb_p}')\n",
    "if nifty50_jb_p < alpha:\n",
    "    print(f\"\"\"Reject H0: Returns are not normally distributed. Accordingly, the project will adopt the following approach:\n",
    "1. Monte-Carlo: Gaussian Monte-Carlo is explicitly rejected.\n",
    "2. XGBoost: Extreme observations will be retained without winsorization and feature engineering will explicitly capture nonlinear and regime-dependent behavior, including rolling volatility and downside-sensitive features.\n",
    "3. Risk Metrics: CVaR will be emphasized over VaR to ensure sensitivity to tail risk.\n",
    "\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"Fail to Reject H0: Returns are normally distributed. Accordingly, the project will adopt the following approach:\n",
    "1. Monte-Carlo: Gaussian Monte-Carlo simulations may be used as a baseline.\n",
    "2. XGBoost: Limited winsorization or robust scaling of extreme observations may be evaluated and feature engineering will emphasize mean-variance dynamics rather than tail-specific assymetries.\n",
    "3. Risk Metrics: Variance-based metrics (standard deviation, Sharpe ratio) may receive greater emphasis, given the absence of strong statistical evidence of tail risk. CVaR may be used for completedness.\n",
    "\"\"\")\n",
    "print('3.2. Anderson-Darling Test: H0: Returns are normally distributed')\n",
    "nifty50_ad = anderson(x, dist='norm')\n",
    "nifty50_ad_stat = nifty50_ad.statistic\n",
    "nifty50_ad_cvalue = nifty50_ad.critical_values[2] # critical value @ 5% significance\n",
    "print(f'Anderson-Darling Test Results: AD-Statistic = {nifty50_ad_stat:.2f} and AD-Critical Value @5% Significance = {nifty50_ad_cvalue}')\n",
    "if nifty50_ad_stat > nifty50_ad_cvalue:\n",
    "    print(f'Reject H0: Returns are not normally distributed. Project approach remains same as outlined in JB above.')\n",
    "else:\n",
    "    print(f'Fail to Reject H0: Returns are normally distributed. Project approach remains same as outlined in JB above.')\n",
    "# 4. Volatility Clustering\n",
    "print('\\n4. Volatility Clustering Tests (ACF and Ljung-Box)')\n",
    "print('4.1. ACF on squared log returns')\n",
    "sq_ret_log = x**2\n",
    "fig_acf, ax_acf = plt.subplots(figsize=(10, 4), dpi=100)\n",
    "plot_acf(\n",
    "    sq_ret_log,\n",
    "    lags=40,\n",
    "    alpha=0.05,\n",
    "    ax=ax_acf\n",
    ")\n",
    "for line in ax_acf.lines:\n",
    "    line.set_color('red')\n",
    "\n",
    "for collections in ax_acf.collections:\n",
    "    collections.set_facecolor('red')\n",
    "    collections.set_alpha(0.3)\n",
    "\n",
    "ax_acf.set_title('ACF on Squared Daily Log Returns - NIFTY50')\n",
    "ax_acf.set_xlabel('Lags')\n",
    "ax_acf.set_ylabel('Autocorrelation')\n",
    "fig_acf.tight_layout()\n",
    "plt.show()\n",
    "print('4.2. Ljung-Box on squared log returns')\n",
    "nifty50_lb = acorr_ljungbox(\n",
    "    sq_ret_log,\n",
    "    lags=[10, 20, 40],\n",
    "    return_df=True\n",
    ")\n",
    "print(f'LB Test:\\n{nifty50_lb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395ab39",
   "metadata": {},
   "source": [
    "**Conclusion - NIFTY50**\n",
    "\n",
    "Statistical diagnostics indicate that NIFTY 50 daily log returns are strongly non-Gaussian, exhibiting pronounced fat tails, negative skewness, and persistent volatility clustering. Normality is decisively rejected by both Jarque–Bera and Anderson–Darling tests, while autocorrelation in squared returns confirms time-varying variance dynamics. Accordingly, Gaussian Monte-Carlo is inappropriate as a standalone risk model, extreme observations are retained for machine learning, volatility-aware features are emphasized, and downside-focused risk measures such as CVaR are prioritized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bd1d2",
   "metadata": {},
   "source": [
    "#### 2.2.2. EDA and Tests on NIFTYBank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1904b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View summary statistics\n",
    "print('SUMMARY STATISTICS')\n",
    "print(f'NIFTYBank dataset - Top 5 and Bottom 5 rows:\\n{niftybank_cudf.head()}\\n{niftybank_cudf.tail()}')\n",
    "print(f'NIFTYBank summary statistics:\\n{niftybank_cudf.describe()}')\n",
    "print('========================================================')\n",
    "print('EDA')\n",
    "# Plot Data\n",
    "fig, ax = plt.subplots(figsize=(40, 15), dpi=100)\n",
    "niftybank_cudf['date'] = cudf.to_datetime(niftybank_cudf['date'])\n",
    "ax.plot(niftybank_cudf['date'], niftybank_cudf['daily_log_closing_value'], '--', color='blue', marker='o')\n",
    "ax.set_title('NIFTY Bank Data Plot - Daily Log Returns', fontsize=30)\n",
    "ax.set_xlabel('Time', fontsize=20)\n",
    "ax.set_ylabel('Daily Log Returns', fontsize=20)\n",
    "ax.set_xlim(niftybank_cudf['date'].min(), niftybank_cudf['date'].max())\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "fig.tight_layout()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "fig_kde, ax_kde = plt.subplots(figsize=(40, 15), dpi=100)\n",
    "sns.kdeplot(niftybank_cudf['daily_log_closing_value'].to_numpy(), bw_adjust=1, bw_method='silverman', color='blue', fill=True, ax=ax_kde)\n",
    "ax_kde.set_title('NIFTY Bank KDE Plot - Daily Log Returns', fontsize=30)\n",
    "ax_kde.set_xlabel('Daily Log Returns', fontsize=20)\n",
    "ax_kde.set_ylabel('Density', fontsize=20)\n",
    "fig_kde.tight_layout()\n",
    "plt.show()\n",
    "print('========================================================')\n",
    "print('STATISTICAL TESTS')\n",
    "# Tests\n",
    "print(f\"\"\"Following statistical tests are being conducted on the data to better understand the distribution guide our way-forward:\n",
    "      1. Kurtosis - to test for tail thickness\n",
    "      2. Skewness - to test for tail assymetry\n",
    "      3. Normality Rejection - Jarque–Bera (moment-based) and Anderson-Darling (tail-sensitive) - to support rejection of Gaussian MC\n",
    "      4. Volatility Clustering - ACF and Ljung-Box on squared returns - to support development of features for XGBoost model\n",
    "\"\"\")\n",
    "# 1. Kurtosis Test\n",
    "print('1. Kurtosis Test')\n",
    "ret_log = niftybank_cudf['daily_log_closing_value'].to_numpy()\n",
    "niftybank_kurt = kurtosis(\n",
    "    ret_log,\n",
    "    fisher=True, # capturing excess kurtosis = pearson kurtosis - 3\n",
    "    bias=False # apply bias correction on estimator\n",
    ")\n",
    "if niftybank_kurt == 0:\n",
    "    print(f'Kurtosis = {niftybank_kurt:.2f}. Outcome - Normal: Tail behavior is consistent with normality')\n",
    "elif niftybank_kurt > 0:\n",
    "    print(f\"\"\"\n",
    "Kurtosis = {niftybank_kurt:.2f}.\n",
    "Outcome:\n",
    "    Distribution is Leptokurtic (fat tails) - meaning there is a higher probability of extreme outcomes and volatility clustering is likely present. Thus, our project will adopt the below approach:\n",
    "    1. Monte-Carlo: During implementation, we will incorporate fat-tailed innovations (e.g., Student-t) to preserve tail behavior.\n",
    "    2. XGBoost: No winsorization of extremes. Outliers shall be retained as-is as they represent genuine market behavior rather than pure noise.\n",
    "    3. Risk Metrics: Conditional Value at Risk (CVaR) - over VaR - will be the preferred metric of choice to ensure sensitivity to tail risk.\n",
    "\"\"\")\n",
    "elif niftybank_kurt < 0:\n",
    "    print(f\"\"\"\n",
    "Kurtosis = {niftybank_kurt:.2f}.\n",
    "Outcome:\n",
    "    Distribution is Platykurtic (thin tails) - meaning there are fewer extreme events relative to a normal distribution. This suggests relatively stable return dynamics with reduced tail risk. Hence, our project will adopt the below approach:\n",
    "    1. Monte-Carlo: Parametric simulations with Gaussian or mildly fat-tailed innovations will be considered.\n",
    "    2. XGBoost: Limited winsorization or other outlier elimination methods shall be evaluated - depending upon the model's out-of-sample performance.\n",
    "    3. Risk Metrics: While CVaR will remain available for completeness, standard deviation and Sharpe-based metrics will receive greater emphasis given the reduced tail risk profile. \n",
    "\"\"\")\n",
    "x = ret_log[np.isfinite(ret_log)]\n",
    "niftybank_kurt_check = kurtosis(x, fisher=True, bias=False)\n",
    "print('Kurtosis sanity check:')\n",
    "if abs(niftybank_kurt - niftybank_kurt_check) < 1e-6:\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Fail')\n",
    "# 2. Skewness Test\n",
    "print('\\n2. Skewness Test')\n",
    "niftybank_skew = skew(\n",
    "    ret_log,\n",
    "    bias=False # apply bias correction on estimator\n",
    ")\n",
    "if niftybank_skew > 0:\n",
    "    print(f\"\"\"\n",
    "Skewness = {niftybank_skew:.2f}.\n",
    "Outcome:\n",
    "    Distribution is positively skewed (right-skewed) – indicating a heavier right tail and a higher frequency of large positive return realizations.\n",
    "    This suggests upside-dominated asymmetry, although tail risk is material since kurtosis is elevated. Accordingly, the project will adopt the following approach:\n",
    "    1. Monte-Carlo: Symmetric or mildly asymmetric fat-tailed innovations may be considered, subject to validation against empirical tails.\n",
    "    2. XGBoost: Retain positive return extremes, as they may capture convex payoff structures or regime-driven upside events.\n",
    "    3. Risk Metrics: CVaR will remain available for completeness, while performance evaluation may also consider upside-sensitive metrics where appropriate.\n",
    "\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"\n",
    "Skewness = {niftybank_skew:.2f}.\n",
    "Outcome:\n",
    "    Distribution is negatively skewed (left-skewed) – indicating a heavier left tail and a higher frequency of large negative returns relative to positive extremes.\n",
    "    This highlights pronounced downside asymmetry and elevated crash risk. Accordingly, the project will adopt the following approach:\n",
    "    1. Monte-Carlo: Consider asymmetric or skew-aware innovations (e.g., skew-t), or focus explicitly on left-tail behavior in simulations.\n",
    "    2. XGBoost: Preserve negative return extremes without winsorization, and prioritize downside-sensitive features (e.g., downside volatility, drawdown-related measures).\n",
    "    3. Risk Metrics: Emphasize left-tail CVaR as the primary risk metric to ensure sensitivity to downside tail risk.\n",
    "\"\"\")\n",
    "niftybank_skew_check = skew(x, bias=False)\n",
    "print('Skewness sanity check:')\n",
    "if abs(niftybank_skew - niftybank_skew_check) < 1e-6:\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Fail')\n",
    "# 3. Normality Rejection Tests (Jarque-Bera and Anderson-Darling)\n",
    "print('\\n3. Normality Rejection Tests (Jarque-Bera and Anderson-Darling)')\n",
    "print('3.1. Jarque-Bera Test: H0: Returns are normally distributed')\n",
    "niftybank_jb_stat, niftybank_jb_p = jarque_bera(x)\n",
    "alpha = 0.05\n",
    "print(f'Jarque-Bera Test Results: JB-Statistic = {niftybank_jb_stat:.2f} and P-Value = {niftybank_jb_p}')\n",
    "if niftybank_jb_p < alpha:\n",
    "    print(f\"\"\"Reject H0: Returns are not normally distributed. Accordingly, the project will adopt the following approach:\n",
    "1. Monte-Carlo: Gaussian Monte-Carlo is explicitly rejected.\n",
    "2. XGBoost: Extreme observations will be retained without winsorization and feature engineering will explicitly capture nonlinear and regime-dependent behavior, including rolling volatility and downside-sensitive features.\n",
    "3. Risk Metrics: CVaR will be emphasized over VaR to ensure sensitivity to tail risk.\n",
    "\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"Fail to Reject H0: Returns are normally distributed. Accordingly, the project will adopt the following approach:\n",
    "1. Monte-Carlo: Gaussian Monte-Carlo simulations may be used as a baseline.\n",
    "2. XGBoost: Limited winsorization or robust scaling of extreme observations may be evaluated and feature engineering will emphasize mean-variance dynamics rather than tail-specific assymetries.\n",
    "3. Risk Metrics: Variance-based metrics (standard deviation, Sharpe ratio) may receive greater emphasis, given the absence of strong statistical evidence of tail risk. CVaR may be used for completedness.\n",
    "\"\"\")\n",
    "print('3.2. Anderson-Darling Test: H0: Returns are normally distributed')\n",
    "niftybank_ad = anderson(x, dist='norm')\n",
    "niftybank_ad_stat = niftybank_ad.statistic\n",
    "niftybank_ad_cvalue = niftybank_ad.critical_values[2] # critical value @ 5% significance\n",
    "print(f'Anderson-Darling Test Results: AD-Statistic = {niftybank_ad_stat:.2f} and AD-Critical Value @5% Significance = {niftybank_ad_cvalue}')\n",
    "if niftybank_ad_stat > niftybank_ad_cvalue:\n",
    "    print(f'Reject H0: Returns are not normally distributed. Project approach remains same as outlined in JB above.')\n",
    "else:\n",
    "    print(f'Fail to Reject H0: Returns are normally distributed. Project approach remains same as outlined in JB above.')\n",
    "# 4. Volatility Clustering\n",
    "print('\\n4. Volatility Clustering Tests (ACF and Ljung-Box)')\n",
    "print('4.1. ACF on squared log returns')\n",
    "sq_ret_log = x**2\n",
    "fig_acf, ax_acf = plt.subplots(figsize=(10, 4), dpi=100)\n",
    "plot_acf(\n",
    "    sq_ret_log,\n",
    "    lags=40,\n",
    "    alpha=0.05,\n",
    "    ax=ax_acf\n",
    ")\n",
    "for line in ax_acf.lines:\n",
    "    line.set_color('blue')\n",
    "\n",
    "for collections in ax_acf.collections:\n",
    "    collections.set_facecolor('blue')\n",
    "    collections.set_alpha(0.3)\n",
    "\n",
    "ax_acf.set_title('ACF on Squared Daily Log Returns - NIFTY Bank')\n",
    "ax_acf.set_xlabel('Lags')\n",
    "ax_acf.set_ylabel('Autocorrelation')\n",
    "fig_acf.tight_layout()\n",
    "plt.show()\n",
    "print('4.2. Ljung-Box on squared log returns')\n",
    "niftybank_lb = acorr_ljungbox(\n",
    "    sq_ret_log,\n",
    "    lags=[10, 20, 40],\n",
    "    return_df=True\n",
    ")\n",
    "print(f'LB Test:\\n{niftybank_lb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414638cc",
   "metadata": {},
   "source": [
    "**Conclusion - NIFTY Bank**\n",
    "\n",
    "Statistical diagnostics indicate that NIFTY Bank daily log returns are strongly non-Gaussian, characterized by pronounced fat tails, clear negative skewness, and highly persistent volatility clustering. Excess kurtosis (9.35) confirms a materially elevated probability of extreme outcomes, while negative skewness (-0.44) highlights asymmetric downside risk and heightened crash sensitivity. Normality is decisively rejected by both Jarque–Bera and Anderson–Darling tests, eliminating Gaussian assumptions. Autocorrelation in squared returns, reinforced by Ljung–Box test rejections across multiple horizons, provides strong evidence of time-varying and persistent volatility dynamics. Accordingly, Gaussian Monte Carlo is unsuitable as a standalone risk framework; simulations must incorporate fat-tailed and potentially skewed innovations, extreme observations should be preserved for machine learning, volatility- and downside-aware features are central to XGBoost modeling, and left-tail-focused risk measures such as CVaR are prioritized over VaR to accurately capture systemic and crash-related risk embedded in NIFTY Bank returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb60316",
   "metadata": {},
   "source": [
    "#### 2.2.3. EDA and Tests on Gold Futures (proxy for Gold spot) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adc176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View summary statistics\n",
    "print('SUMMARY STATISTICS')\n",
    "print(f'Gold Futures dataset - Top 5 and Bottom 5 rows:\\n{goldfutures_cudf.head()}\\n{goldfutures_cudf.tail()}')\n",
    "print(f'Gold Futures summary statistics:\\n{goldfutures_cudf.describe()}')\n",
    "print('========================================================')\n",
    "print('EDA')\n",
    "# Plot Data\n",
    "fig, ax = plt.subplots(figsize=(40, 15), dpi=100)\n",
    "print(f'Date datatype:{goldfutures_cudf['date'].dtype}')\n",
    "goldfutures_cudf['date'] = cudf.to_datetime(goldfutures_cudf['date'])\n",
    "ax.plot(goldfutures_cudf['date'], goldfutures_cudf['daily_log_closing_value'], '--', color='black', marker='o')\n",
    "ax.set_title('Gold Futures Data Plot - Daily Log Returns', fontsize=30)\n",
    "ax.set_xlabel('Time', fontsize=20)\n",
    "ax.set_ylabel('Daily Log Returns', fontsize=20)\n",
    "ax.set_xlim(goldfutures_cudf['date'].min(), goldfutures_cudf['date'].max())\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator([1, 4, 7, 10]))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "fig.tight_layout()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "fig_kde, ax_kde = plt.subplots(figsize=(40, 15), dpi=100)\n",
    "sns.kdeplot(goldfutures_cudf['daily_log_closing_value'].to_numpy(), bw_adjust=1, bw_method='silverman', color='black', fill=True, ax=ax_kde)\n",
    "ax_kde.set_title('Gold Futures KDE Plot - Daily Log Returns', fontsize=30)\n",
    "ax_kde.set_xlabel('Daily Log Returns', fontsize=20)\n",
    "ax_kde.set_ylabel('Density', fontsize=20)\n",
    "fig_kde.tight_layout()\n",
    "plt.show()\n",
    "print('========================================================')\n",
    "print('STATISTICAL TESTS')\n",
    "# Tests\n",
    "print(f\"\"\"Following statistical tests are being conducted on the data to better understand the distribution guide our way-forward:\n",
    "      1. Kurtosis - to test for tail thickness\n",
    "      2. Skewness - to test for tail assymetry\n",
    "      3. Normality Rejection - Jarque–Bera (moment-based) and Anderson-Darling (tail-sensitive) - to support rejection of Gaussian MC\n",
    "      4. Volatility Clustering - ACF and Ljung-Box on squared returns - to support development of features for XGBoost model\n",
    "\"\"\")\n",
    "# 1. Kurtosis Test\n",
    "print('1. Kurtosis Test')\n",
    "ret_log = goldfutures_cudf['daily_log_closing_value'].to_numpy()\n",
    "goldfutures_kurt = kurtosis(\n",
    "    ret_log,\n",
    "    fisher=True, # capturing excess kurtosis = pearson kurtosis - 3\n",
    "    bias=False # apply bias correction on estimator\n",
    ")\n",
    "if goldfutures_kurt == 0:\n",
    "    print(f'Kurtosis = {goldfutures_kurt:.2f}. Outcome - Normal: Tail behavior is consistent with normality')\n",
    "elif goldfutures_kurt > 0:\n",
    "    print(f\"\"\"\n",
    "Kurtosis = {goldfutures_kurt:.2f}.\n",
    "Outcome:\n",
    "    Distribution is Leptokurtic (fat tails) - meaning there is a higher probability of extreme outcomes and volatility clustering is likely present. Thus, our project will adopt the below approach:\n",
    "    1. Monte-Carlo: During implementation, we will incorporate fat-tailed innovations (e.g., Student-t) to preserve tail behavior.\n",
    "    2. XGBoost: No winsorization of extremes. Outliers shall be retained as-is as they represent genuine market behavior rather than pure noise.\n",
    "    3. Risk Metrics: Conditional Value at Risk (CVaR) - over VaR - will be the preferred metric of choice to ensure sensitivity to tail risk.\n",
    "\"\"\")\n",
    "elif goldfutures_kurt < 0:\n",
    "    print(f\"\"\"\n",
    "Kurtosis = {goldfutures_kurt:.2f}.\n",
    "Outcome:\n",
    "    Distribution is Platykurtic (thin tails) - meaning there are fewer extreme events relative to a normal distribution. This suggests relatively stable return dynamics with reduced tail risk. Hence, our project will adopt the below approach:\n",
    "    1. Monte-Carlo: Parametric simulations with Gaussian or mildly fat-tailed innovations will be considered.\n",
    "    2. XGBoost: Limited winsorization or other outlier elimination methods shall be evaluated - depending upon the model's out-of-sample performance.\n",
    "    3. Risk Metrics: While CVaR will remain available for completeness, standard deviation and Sharpe-based metrics will receive greater emphasis given the reduced tail risk profile. \n",
    "\"\"\")\n",
    "x = ret_log[np.isfinite(ret_log)]\n",
    "goldfutures_kurt_check = kurtosis(x, fisher=True, bias=False)\n",
    "print('Kurtosis sanity check:')\n",
    "if abs(goldfutures_kurt - goldfutures_kurt_check) < 1e-6:\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Fail')\n",
    "# 2. Skewness Test\n",
    "print('\\n2. Skewness Test')\n",
    "goldfutures_skew = skew(\n",
    "    ret_log,\n",
    "    bias=False # apply bias correction on estimator\n",
    ")\n",
    "if goldfutures_skew > 0:\n",
    "    print(f\"\"\"\n",
    "Skewness = {goldfutures_skew:.2f}.\n",
    "Outcome:\n",
    "    Distribution is positively skewed (right-skewed) – indicating a heavier right tail and a higher frequency of large positive return realizations.\n",
    "    This suggests upside-dominated asymmetry, although tail risk is material since kurtosis is elevated. Accordingly, the project will adopt the following approach:\n",
    "    1. Monte-Carlo: Symmetric or mildly asymmetric fat-tailed innovations may be considered, subject to validation against empirical tails.\n",
    "    2. XGBoost: Retain positive return extremes, as they may capture convex payoff structures or regime-driven upside events.\n",
    "    3. Risk Metrics: CVaR will remain available for completeness, while performance evaluation may also consider upside-sensitive metrics where appropriate.\n",
    "\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"\n",
    "Skewness = {goldfutures_skew:.2f}.\n",
    "Outcome:\n",
    "    Distribution is negatively skewed (left-skewed) – indicating a heavier left tail and a higher frequency of large negative returns relative to positive extremes.\n",
    "    This highlights pronounced downside asymmetry and elevated crash risk. Accordingly, the project will adopt the following approach:\n",
    "    1. Monte-Carlo: Consider asymmetric or skew-aware innovations (e.g., skew-t), or focus explicitly on left-tail behavior in simulations.\n",
    "    2. XGBoost: Preserve negative return extremes without winsorization, and prioritize downside-sensitive features (e.g., downside volatility, drawdown-related measures).\n",
    "    3. Risk Metrics: Emphasize left-tail CVaR as the primary risk metric to ensure sensitivity to downside tail risk.\n",
    "\"\"\")\n",
    "goldfutures_skew_check = skew(x, bias=False)\n",
    "print('Skewness sanity check:')\n",
    "if abs(goldfutures_skew - goldfutures_skew_check) < 1e-6:\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Fail')\n",
    "# 3. Normality Rejection Tests (Jarque-Bera and Anderson-Darling)\n",
    "print('\\n3. Normality Rejection Tests (Jarque-Bera and Anderson-Darling)')\n",
    "print('3.1. Jarque-Bera Test: H0: Returns are normally distributed')\n",
    "goldfutures_jb_stat, goldfutures_jb_p = jarque_bera(x)\n",
    "alpha = 0.05\n",
    "print(f'Jarque-Bera Test Results: JB-Statistic = {goldfutures_jb_stat:.2f} and P-Value = {goldfutures_jb_p}')\n",
    "if goldfutures_jb_p < alpha:\n",
    "    print(f\"\"\"Reject H0: Returns are not normally distributed. Accordingly, the project will adopt the following approach:\n",
    "1. Monte-Carlo: Gaussian Monte-Carlo is explicitly rejected.\n",
    "2. XGBoost: Extreme observations will be retained without winsorization and feature engineering will explicitly capture nonlinear and regime-dependent behavior, including rolling volatility and downside-sensitive features.\n",
    "3. Risk Metrics: CVaR will be emphasized over VaR to ensure sensitivity to tail risk.\n",
    "\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"Fail to Reject H0: Returns are normally distributed. Accordingly, the project will adopt the following approach:\n",
    "1. Monte-Carlo: Gaussian Monte-Carlo simulations may be used as a baseline.\n",
    "2. XGBoost: Limited winsorization or robust scaling of extreme observations may be evaluated and feature engineering will emphasize mean-variance dynamics rather than tail-specific assymetries.\n",
    "3. Risk Metrics: Variance-based metrics (standard deviation, Sharpe ratio) may receive greater emphasis, given the absence of strong statistical evidence of tail risk. CVaR may be used for completedness.\n",
    "\"\"\")\n",
    "print('3.2. Anderson-Darling Test: H0: Returns are normally distributed')\n",
    "goldfutures_ad = anderson(x, dist='norm')\n",
    "goldfutures_ad_stat = goldfutures_ad.statistic\n",
    "goldfutures_ad_cvalue = goldfutures_ad.critical_values[2] # critical value @ 5% significance\n",
    "print(f'Anderson-Darling Test Results: AD-Statistic = {goldfutures_ad_stat:.2f} and AD-Critical Value @5% Significance = {goldfutures_ad_cvalue}')\n",
    "if goldfutures_ad_stat > goldfutures_ad_cvalue:\n",
    "    print(f'Reject H0: Returns are not normally distributed. Project approach remains same as outlined in JB above.')\n",
    "else:\n",
    "    print(f'Fail to Reject H0: Returns are normally distributed. Project approach remains same as outlined in JB above.')\n",
    "# 4. Volatility Clustering\n",
    "print('\\n4. Volatility Clustering Tests (ACF and Ljung-Box)')\n",
    "print('4.1. ACF on squared log returns')\n",
    "sq_ret_log = x**2\n",
    "fig_acf, ax_acf = plt.subplots(figsize=(10, 4), dpi=100)\n",
    "plot_acf(\n",
    "    sq_ret_log,\n",
    "    lags=40,\n",
    "    alpha=0.05,\n",
    "    ax=ax_acf\n",
    ")\n",
    "for line in ax_acf.lines:\n",
    "    line.set_color('black')\n",
    "\n",
    "for collections in ax_acf.collections:\n",
    "    collections.set_facecolor('black')\n",
    "    collections.set_alpha(0.3)\n",
    "\n",
    "ax_acf.set_title('ACF on Squared Daily Log Returns - Gold Futures')\n",
    "ax_acf.set_xlabel('Lags')\n",
    "ax_acf.set_ylabel('Autocorrelation')\n",
    "fig_acf.tight_layout()\n",
    "plt.show()\n",
    "print('4.2. Ljung-Box on squared log returns')\n",
    "goldfutures_lb = acorr_ljungbox(\n",
    "    sq_ret_log,\n",
    "    lags=[10, 20, 40],\n",
    "    return_df=True\n",
    ")\n",
    "print(f'LB Test:\\n{goldfutures_lb}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2660a",
   "metadata": {},
   "source": [
    "**Conclusion - Gold Futures (COMEX Proxy for Spot Gold)**\n",
    "\n",
    "Statistical diagnostics indicate that Gold Futures daily log returns are distinctly non-Gaussian, exhibiting moderate but economically meaningful fat tails, mild negative skewness, and persistent—though comparatively smoother—volatility clustering. Excess kurtosis (5.14) confirms an elevated probability of extreme outcomes relative to a normal distribution, while negative skewness (-0.32) highlights asymmetric downside risk, albeit less severe than in equity indices. Normality is decisively rejected by both Jarque–Bera and Anderson–Darling tests, ruling out Gaussian assumptions for simulation-based risk modeling. Autocorrelation in squared returns, reinforced by Ljung–Box test rejections across multiple horizons, provides strong evidence of time-varying volatility dynamics. Accordingly, Gaussian Monte Carlo is inappropriate for Gold Futures, simulations must incorporate fat-tailed (and potentially skewed) innovations, extreme observations should be retained for machine learning models, volatility-aware and downside-sensitive features are emphasized in XGBoost, and CVaR is preferred over VaR to ensure adequate capture of tail risk—even in an asset traditionally perceived as a defensive or stabilizing component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1355bd",
   "metadata": {},
   "source": [
    "#### 2.2.4. EDA and Tests Conclusion:\n",
    "\n",
    "*(NIFTY 50, NIFTY Bank, Gold Futures)*\n",
    "\n",
    "Across equities and commodities, the statistical evidence consistently shows that **asset returns deviate materially from Gaussian assumptions**. All three assets exhibit fat tails, downside asymmetry, and time-varying volatility, with the strength of these effects differing by asset class. NIFTY Bank displays the most severe tail and crash risk, NIFTY 50 shows structurally non-Gaussian behavior typical of broad equity markets, and Gold—while comparatively more stable—still exhibits meaningful departures from normality. These findings directly shape the modeling strategy for this project.\n",
    "\n",
    "##### Implications for Monte-Carlo Simulation\n",
    "- **Gaussian Monte-Carlo is rejected as a standalone approach** across all assets, as it fails to capture observed tail risk and volatility dynamics.\n",
    "- Simulations will incorporate **fat-tailed innovations** (e.g., Student-t), with flexibility to allow for **asymmetry where downside risk is pronounced**, particularly for equity indices.\n",
    "- Monte-Carlo outputs will be interpreted as **risk-scenario generators rather than precise distributional forecasts**, recognizing regime dependence and structural breaks in financial markets.\n",
    "- Gold simulations, while still non-Gaussian, will reflect **relatively lower tail severity**, supporting its role as a stabilizing portfolio component without assuming normality.\n",
    "\n",
    "##### Implications for XGBoost and Machine Learning\n",
    "- **Extreme observations are retained** across all assets; no winsorization or truncation is applied, as extremes represent genuine market behavior.\n",
    "- Feature engineering emphasizes **volatility-aware and downside-sensitive variables**, including rolling volatility, lagged returns, and drawdown-related measures.\n",
    "- Volatility clustering evidenced by ACF and Ljung-Box tests justifies the use of **lagged and rolling features** to capture persistence and regime effects.\n",
    "- XGBoost is positioned as a **complementary, data-driven alternative** to simulation models, particularly suited to capturing nonlinearities and cross-asset differences in risk behavior.\n",
    "\n",
    "##### Implications for Risk Metrics and Portfolio Evaluation\n",
    "- **CVaR is prioritized over VaR** as the primary risk metric across all assets due to its sensitivity to tail losses.\n",
    "- Downside-focused risk evaluation is emphasized, especially for NIFTY Bank, where crash risk and tail dependence are structurally higher.\n",
    "- Risk comparisons across assets explicitly account for **differences in tail behavior**, rather than relying solely on volatility or standard deviation.\n",
    "- Gold’s contribution to portfolios is assessed not by assumed normality, but by its **empirically observed tail characteristics and volatility persistence**, reinforcing its diversification role on a risk-adjusted basis.\n",
    "\n",
    "##### Overall Project Positioning\n",
    "Taken together, the diagnostics support a **hybrid risk-modeling framework**:  \n",
    "- **Non-Gaussian Monte-Carlo** for scenario-based risk exploration,  \n",
    "- **XGBoost** for capturing nonlinear, volatility-driven dynamics, and  \n",
    "- **CVaR-focused evaluation** for consistent, downside-aware risk assessment.\n",
    "\n",
    "This integrated approach ensures that portfolio risk is evaluated using models and metrics that are **aligned with observed market behavior**, rather than imposed theoretical assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3bd0a2",
   "metadata": {},
   "source": [
    "## **Section 3: Feature Engineering for XGBoost**\n",
    "\n",
    "Based on the findings from section 2, this section focusses on undertaking feature engineering - across all 3 datasets - to generate analysis-ready files. This section starts by outlining guiding principles for identification of the engineered features and then proceeds to implement the same.\n",
    "\n",
    "### XGBoost Feature Engineering Guiding Principles\n",
    "\n",
    "**Features Identified Based on Statistical Tests:**\n",
    "\n",
    " - **Return-based Features:** Statistical diagnosis suggest Non-Gaussian returns and volatility clustering meaning that simple linear/constant-variance assumptions are insufficient. Based on this finding, return-based features need to include lagged and rolling features to capture persistence and regime shifts and thus, the following features will be implemented:\n",
    "     - Lagged Returns:\n",
    "        - Lag 1: $r_{t-1}$\n",
    "        - Lag 2: $r_{t-2}$\n",
    "        - Lag 3: $r_{t-3}$\n",
    "        - Lag 4: $r_{t-4}$\n",
    "        - Lag 5: $r_{t-5}$\n",
    "        - Lag 10: $r_{t-10}$\n",
    "        - Lag 20: $r_{t-20}$\n",
    "     - Rolling Mean Returns:\n",
    "        - Rolling Mean of Returns over:\n",
    "            - 5 days\n",
    "            - 10 days\n",
    "            - 20 days\n",
    "     - Return Momentum:\n",
    "        - Cumulative Returns over:\n",
    "            - 5 days\n",
    "            - 10 days\n",
    "            - 20 days\n",
    "     - Volatility Persistence Proxies:\n",
    "        - Squared Returns: $r²_t$\n",
    "        - Lagged Squared Returns:\n",
    "            - Squared Lag 1: $r²_{t-1}$\n",
    "            - Squared Lag 5: $r²_{t-5}$\n",
    "            - Squared Lag 10: $r²_{t-10}$\n",
    "        - Rolling Mean of Squared Returns over:\n",
    "            - 5 days\n",
    "            - 10 days\n",
    " - **Volatility Clustering Features:** Statistical diagnosis - directly motivated by ACF and Ljung-Box tests - suggest presence of time-varying volatility (with strength varying across the 3 asset classes). Hence, the following features will be implemented (with strong focus on volatility ratios - which will help XGBoost identify and handle volatility regime shifts):\n",
    "     - Rolling Volatility:\n",
    "        - Rolling standard deviation of returns over:\n",
    "            - 5 days\n",
    "            - 10 days\n",
    "            - 20 days\n",
    "            - 40 days\n",
    "            - 60 days\n",
    "     - Lagged Volatility (over 20 days):\n",
    "        - Lag 1: $\\sigma_{20,t-1}$\n",
    "        - Lag 5: $\\sigma_{20,t-5}$\n",
    "        - Lag 10: $\\sigma_{20,t-10}$\n",
    "     - Volatility Ratios (for Regime Detection):\n",
    "        - Short v/s Medium (near-term stress): $\\sigma_{5,t} / \\sigma_{20,t}$\n",
    "        - Medium v/s Long (regime shift): $\\sigma_{20,t} / \\sigma_{60,t}$\n",
    " - **Downside Sensitive Features:** Statistical diagnosis identified negative skew across all 3 asset classes. This explicilty warrants the presence of dedicated downside sensitive features for XGBoost to be correctly modelled. Hence, the following features will be implemented:\n",
    "     - Negative Return Indicators:\n",
    "        - Binary Downside Flag: $I(r_t < 0)$\n",
    "        - Severe Downside Flag: $I(r_t < Q_{0.05,t}^{(20)})$ where $Q_{0.05,t}^{(20)}$ is the 0.05 (5%) rolling quantile over a 20-day window.\n",
    "     - Downside Volatility:\n",
    "        - Rolling downside standard deviation (returns < 0) over:\n",
    "            - 10 days\n",
    "            - 20 days\n",
    "     - Rolling Drawdowns:\n",
    "        - Maximum drawdown over:\n",
    "            - 10 days\n",
    "            - 20 days\n",
    "            - 60 days\n",
    " - **Tail-Risk Proxies:** With the statistical diagnosis credibly establishing that all 3 datasets are Leptokurtic, it is imperative for the final dataset to have features that can allow XGBoost to learn tail dynamics indirectly. Hence, the following features will be implemented:\n",
    "     - Rolling Quantiles:\n",
    "        - 1% rolling quantile (20-day window)\n",
    "        - 5% rolling quantile (20-day window)\n",
    "     - Extreme Move Indicators:\n",
    "        - Absolute Return Percentile: $I(|r_t| > Q_{0.95,t}^{(20)}(|r|))$ where $Q_{0.95,t}^{(20)}(|r|)$ is the 0.95 (95%) rolling quantile over a 20-day window.\n",
    "     - Volatility Adjusted Returns:\n",
    "        - Standardized Return: $r_t / \\sigma_{20}$\n",
    "\n",
    " **Features Identified to Boost Model Performance:**\n",
    " - **Regime Identification Features:** XGBoost performs best when regimes are learned, not imposed. Hence, the following features will be implemented:\n",
    "     - *Volatility Regime Flags:*\n",
    "        - High volatility regime: $\\sigma_{20} > rolling median(\\sigma_{20})$\n",
    "        - Stress regime: $\\sigma_{5} > \\sigma_{20} AND r_t < 0$\n",
    "     - *Trend + Volatility Interaction:*\n",
    "        - Return x Volatility: $r_t * \\sigma_{20}$\n",
    "        - Downside x Volatility: $min(r_t, 0) * \\sigma_{20}$\n",
    " - **Calendar Features:** While these are not core drivers, they will provide critical controls to the model and hence, the following features will be implemented:\n",
    "     - *Day-of-the-week: One-hot encoded*\n",
    "     - *Separate month identifier column*\n",
    "     - *End-of-month flag*\n",
    "\n",
    "In total, 47 features will be engineered for the XGBoost model - which is a reasonable count for XGBoost given the long daily history and the need to capture volatility regimes and tail behavior. Tree-based ensembles handle correlated predictors well, and feature importance diagnostics will be used post-fit to validate contribution and prune if needed (revised list of features shall be included (in the Appendix) - if pruning is conducted). Additionally, each feature family has been designed based on:\n",
    " - fat tails\n",
    " - negative skew\n",
    " - volatility clustering\n",
    "Which have been empirically demonstrated in our statistical diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac41c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineered master data:\n",
      "   SNo.       date  closing_value  daily_closing_pct_change  \\\n",
      "0     1 2001-01-02         1271.8                  0.013952   \n",
      "1     2 2001-01-03         1291.2                  0.015254   \n",
      "2     3 2001-01-04         1307.6                  0.012701   \n",
      "3     4 2001-01-05         1327.2                  0.014989   \n",
      "4     5 2001-01-08         1309.2                 -0.013562   \n",
      "\n",
      "   daily_log_closing_value  instrument_token trading_symbol exhange_name  \\\n",
      "0                 0.013856          256265.0       NIFTY 50         None   \n",
      "1                 0.015139          256265.0       NIFTY 50         None   \n",
      "2                 0.012621          256265.0       NIFTY 50         None   \n",
      "3                 0.014878          256265.0       NIFTY 50         None   \n",
      "4                -0.013655          256265.0       NIFTY 50         None   \n",
      "\n",
      "   return_lag_1  return_lag_2  ...  day_of_week  dow_0  dow_1  dow_2  dow_3  \\\n",
      "0           NaN           NaN  ...            1      0      1      0      0   \n",
      "1      0.013856           NaN  ...            2      0      0      1      0   \n",
      "2      0.015139      0.013856  ...            3      0      0      0      1   \n",
      "3      0.012621      0.015139  ...            4      0      0      0      0   \n",
      "4      0.014878      0.012621  ...            0      1      0      0      0   \n",
      "\n",
      "   dow_4  dow_5  dow_6  month_identifier  end_of_month_flag  \n",
      "0      0      0      0                 1                  0  \n",
      "1      0      0      0                 1                  0  \n",
      "2      0      0      0                 1                  0  \n",
      "3      1      0      0                 1                  0  \n",
      "4      0      0      0                 1                  0  \n",
      "\n",
      "[5 rows x 62 columns]\n",
      "Loaded 18699 records to csv.\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering implementation - vertical merge to create a consolidated 3 asset master data sheet\n",
    "xgb_master_data_cudf = cudf.concat([nifty50_cudf, niftybank_cudf, goldfutures_cudf])\n",
    "\n",
    "# Comverting dataframe to CPU environment - for easier implementation\n",
    "xgb_master_data_df = xgb_master_data_cudf.to_pandas()\n",
    "\n",
    "# Implementing return-based features\n",
    "\n",
    "# Generating lagged returns (1, 2, 3, 4, 5, 10 and 20)\n",
    "xgb_master_data_df['return_lag_1'] = xgb_master_data_df['daily_log_closing_value'].shift(1)\n",
    "xgb_master_data_df['return_lag_2'] = xgb_master_data_df['daily_log_closing_value'].shift(2)\n",
    "xgb_master_data_df['return_lag_3'] = xgb_master_data_df['daily_log_closing_value'].shift(3)\n",
    "xgb_master_data_df['return_lag_4'] = xgb_master_data_df['daily_log_closing_value'].shift(4)\n",
    "xgb_master_data_df['return_lag_5'] = xgb_master_data_df['daily_log_closing_value'].shift(5)\n",
    "xgb_master_data_df['return_lag_10'] = xgb_master_data_df['daily_log_closing_value'].shift(10)\n",
    "xgb_master_data_df['return_lag_20'] = xgb_master_data_df['daily_log_closing_value'].shift(20)\n",
    "\n",
    "# Generating rolling mean returns (5, 10 and 20)\n",
    "xgb_master_data_df['return_ma_5'] = xgb_master_data_df['daily_log_closing_value'].rolling(5).mean()\n",
    "xgb_master_data_df['return_ma_10'] = xgb_master_data_df['daily_log_closing_value'].rolling(10).mean()\n",
    "xgb_master_data_df['return_ma_20'] = xgb_master_data_df['daily_log_closing_value'].rolling(20).mean()\n",
    "\n",
    "# Generating cumulative returns (5, 10 and 20)\n",
    "xgb_master_data_df['return_cumulative_5'] = xgb_master_data_df['daily_log_closing_value'].expanding(5).mean()\n",
    "xgb_master_data_df['return_cumulative_10'] = xgb_master_data_df['daily_log_closing_value'].expanding(10).mean()\n",
    "xgb_master_data_df['return_cumulative_20'] = xgb_master_data_df['daily_log_closing_value'].expanding(20).mean()\n",
    "\n",
    "# Generating squared returns\n",
    "xgb_master_data_df['squared_daily_log_closing_value'] = xgb_master_data_df['daily_log_closing_value'] ** 2\n",
    "\n",
    "# Generating lagged square returns (1, 5 and 10)\n",
    "xgb_master_data_df['squared_return_lag_1'] = xgb_master_data_df['squared_daily_log_closing_value'].shift(1)\n",
    "xgb_master_data_df['squared_return_lag_5'] = xgb_master_data_df['squared_daily_log_closing_value'].shift(5)\n",
    "xgb_master_data_df['squared_return_lag_10'] = xgb_master_data_df['squared_daily_log_closing_value'].shift(10)\n",
    "\n",
    "# Generating rolling mean of squared returns (5 and 10)\n",
    "xgb_master_data_df['squared_return_ma_5'] = xgb_master_data_df['squared_daily_log_closing_value'].rolling(5).mean()\n",
    "xgb_master_data_df['squared_return_ma_5'] = xgb_master_data_df['squared_daily_log_closing_value'].rolling(10).mean()\n",
    "\n",
    "# Implementing volatility clustering features\n",
    "\n",
    "# Generating rolling standard deviation returns (5, 10, 20, 40 and 60)\n",
    "xgb_master_data_df['return_vol_ma_5'] = xgb_master_data_df['daily_log_closing_value'].rolling(5).std()\n",
    "xgb_master_data_df['return_vol_ma_10'] = xgb_master_data_df['daily_log_closing_value'].rolling(10).std()\n",
    "xgb_master_data_df['return_vol_ma_20'] = xgb_master_data_df['daily_log_closing_value'].rolling(20).std()\n",
    "xgb_master_data_df['return_vol_ma_40'] = xgb_master_data_df['daily_log_closing_value'].rolling(40).std()\n",
    "xgb_master_data_df['return_vol_ma_60'] = xgb_master_data_df['daily_log_closing_value'].rolling(60).std()\n",
    "\n",
    "# Generating lagged volatility (1, 5 and 10) over 20 days\n",
    "xgb_master_data_df['return_vol_ma_20_lag_1'] = xgb_master_data_df['return_vol_ma_20'].shift(1)\n",
    "xgb_master_data_df['return_vol_ma_20_lag_5'] = xgb_master_data_df['return_vol_ma_20'].shift(5)\n",
    "xgb_master_data_df['return_vol_ma_20_lag_10'] = xgb_master_data_df['return_vol_ma_20'].shift(10)\n",
    "\n",
    "# Generating volatility ratios\n",
    "xgb_master_data_df['short_medium_lag_5_lag_20'] = xgb_master_data_df['return_vol_ma_5'] / xgb_master_data_df['return_vol_ma_20']\n",
    "xgb_master_data_df['medium_long_lag_20_lag_60'] = xgb_master_data_df['return_vol_ma_20'] / xgb_master_data_df['return_vol_ma_60']\n",
    "\n",
    "# Generating binary downside flags\n",
    "log_returns_list = xgb_master_data_df['daily_log_closing_value'].to_list()\n",
    "binary_flag = []\n",
    "for i in range(len(log_returns_list)):\n",
    "    if log_returns_list[i] < 0:\n",
    "        binary_flag.append(1)\n",
    "    else:\n",
    "        binary_flag.append(0)\n",
    "\n",
    "xgb_master_data_df['binary_downside_flag'] = pd.Series(binary_flag)\n",
    "\n",
    "# Generating severe downside flags and adding rolling quantiles (5% and 1% over 20-days) for tail risk\n",
    "xgb_master_data_df['return_q05_20'] = xgb_master_data_df['daily_log_closing_value'].rolling(20).quantile(0.05) # 5% rolling quantile over a 20 day period (meets the requirement of rolling quantiles for tail risk proxies)\n",
    "xgb_master_data_df['return_q01_20'] = xgb_master_data_df['daily_log_closing_value'].rolling(20).quantile(0.01) # 5% rolling quantile over a 20 day period (meets the requirement of rolling quantiles for tail risk proxies)\n",
    "rolling_quantile_return_5 = xgb_master_data_df['return_q05_20'].to_list()\n",
    "severe_downside_flag = []\n",
    "for i in range(len(rolling_quantile_return_5)):\n",
    "    if log_returns_list[i] < rolling_quantile_return_5[i]:\n",
    "        severe_downside_flag.append(1)\n",
    "    else:\n",
    "        severe_downside_flag.append(0)\n",
    "\n",
    "xgb_master_data_df['severe_downside_flag'] = pd.Series(severe_downside_flag)\n",
    "\n",
    "# Generating downside volatility - rolling downside standard deviation (over 10 and 20 days)\n",
    "# Steps to compute rolling downside volatility:\n",
    "# 1. Take last 20 daily log returns \n",
    "# 2. Identify daily log returns < 0 \n",
    "# 3. Compute std deviation of the subset\n",
    "log_returns_series = xgb_master_data_df['daily_log_closing_value']\n",
    "\n",
    "def downside_vol(window: pd.Series):\n",
    "    neg_ret = window[window<0]\n",
    "    if neg_ret.size >= 2:\n",
    "        return float(neg_ret.std(ddof=1))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "xgb_master_data_df['rolling_downside_stddev_10'] = log_returns_series.rolling(10).apply(downside_vol, raw=False) # raw = False since we are passing a series - set this to True if an array is to be passed.\n",
    "xgb_master_data_df['rolling_downside_stddev_20'] = log_returns_series.rolling(20).apply(downside_vol, raw=False) # raw = False since we are passing a series - set this to True if an array is to be passed.\n",
    "\n",
    "# Generating rolling max drawdown (largest peak to trough %age loss) over 10, 20 and 60 days\n",
    "# Steps to compute max drawdown\n",
    "# 1. Build an array consisting of cumulative sum values of the log returns\n",
    "# 2. Take exp of cumulative sum of log returns array to get an array comprising of simple return index (simple return index = 1 + daily %age change) values\n",
    "# 3. Take the cumulative maximum of the array above - this will be the peak \n",
    "# 4. Take %age change of each value (in the simple return index array) w.r.t the peak computed above to create the drawdown series \n",
    "# 5. Since the drawdown series will comprise of -ve values, we will take the min of the series (maximum absolute figure) and return the same\n",
    "def max_drawdown(window: pd.Series):\n",
    "    cum_log_array = window.cumsum()\n",
    "    cum_simple_array = np.exp(cum_log_array)\n",
    "    peak = cum_simple_array.cummax()\n",
    "    drawdowns = cum_simple_array / peak - 1\n",
    "    value = drawdowns.min()\n",
    "    return value\n",
    "\n",
    "xgb_master_data_df['rolling_max_drawdown_10'] = log_returns_series.rolling(10).apply(max_drawdown, raw=False) # raw = False since we are passing a series - set this to True if an array is to be passed.\n",
    "xgb_master_data_df['rolling_max_drawdown_20'] = log_returns_series.rolling(20).apply(max_drawdown, raw=False) # raw = False since we are passing a series - set this to True if an array is to be passed.\n",
    "xgb_master_data_df['rolling_max_drawdown_60'] = log_returns_series.rolling(60).apply(max_drawdown, raw=False) # raw = False since we are passing a series - set this to True if an array is to be passed.\n",
    "\n",
    "# Generating tail risk proxies\n",
    "# Absolute return percentile\n",
    "log_returns_series_abs = xgb_master_data_df['daily_log_closing_value'].abs()\n",
    "return_q95_20 = log_returns_series_abs.rolling(20).quantile(0.95) # 95% rolling quantile on absolute log returns - over 20 day window\n",
    "xgb_master_data_df['abs_return_percentile_indicator'] = (log_returns_series_abs > return_q95_20).astype('int')\n",
    "# Volatility adjusted return - standardized return\n",
    "xgb_master_data_df['standardized_return'] = xgb_master_data_df['daily_log_closing_value'] /  xgb_master_data_df['return_vol_ma_20'].where(xgb_master_data_df['return_vol_ma_20'] > 0)\n",
    "\n",
    "# Generating regime identification features\n",
    "# Volatility regime flags - High volatility regime\n",
    "xgb_master_data_df['rolling_vol_median_20'] =  xgb_master_data_df['return_vol_ma_20'].rolling(20).median()\n",
    "xgb_master_data_df['high_vol_regime_flag'] = (\n",
    "    xgb_master_data_df['return_vol_ma_20'] > xgb_master_data_df['rolling_vol_median_20']\n",
    ").astype('int')\n",
    "# Volatility regime flags - Stress regime\n",
    "xgb_master_data_df['stress_regime_flag'] = (\n",
    "    (xgb_master_data_df['return_vol_ma_5'] > xgb_master_data_df['return_vol_ma_20']) & (xgb_master_data_df['daily_log_closing_value'] < 0)\n",
    ").astype('int')\n",
    "# Trend + volatility interaction - Return x Volatility\n",
    "xgb_master_data_df['trend_vol_return_vol_ma_20'] = xgb_master_data_df['daily_log_closing_value'] * xgb_master_data_df['return_vol_ma_20']\n",
    "# Trend + volatility interaction - Downside x Volatility\n",
    "min_returns = xgb_master_data_df['daily_log_closing_value'].where(\n",
    "    xgb_master_data_df['daily_log_closing_value'] < 0,\n",
    "    0\n",
    ")\n",
    "xgb_master_data_df['downside_vol_return_vol_ma_20'] = min_returns * xgb_master_data_df['return_vol_ma_20']\n",
    "\n",
    "# Generating calendar features\n",
    "# Day of the week - one hot encoded\n",
    "xgb_master_data_df['date'] = pd.to_datetime(xgb_master_data_df['date'])\n",
    "xgb_master_data_df['day_of_week'] = xgb_master_data_df['date'].dt.dayofweek\n",
    "dow_encoding = pd.get_dummies(\n",
    "    xgb_master_data_df['day_of_week'],\n",
    "    prefix='dow'\n",
    ").astype('int')\n",
    "xgb_master_data_df = pd.concat(\n",
    "    [xgb_master_data_df,\n",
    "    dow_encoding],\n",
    "    axis = 1\n",
    ")\n",
    "# Month identifier\n",
    "xgb_master_data_df['month_identifier'] = xgb_master_data_df['date'].dt.month\n",
    "# End of month flag\n",
    "xgb_master_data_df['end_of_month_flag'] = (\n",
    "    xgb_master_data_df['date'].dt.is_month_end\n",
    ").astype('int')\n",
    "\n",
    "print(f'Feature engineered master data:\\n{xgb_master_data_df.head()}')\n",
    "xgb_master_data_df.to_csv(xgb_master_data)\n",
    "print(f'Loaded {len(xgb_master_data_df)} records to csv.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cafdd7",
   "metadata": {},
   "source": [
    "## **Section 4: Portfolio Construction Rule - Maximizing Sharpe Ratio**\n",
    "\n",
    "A **portfolio construction rule** defines how expected returns and risk estimates are translated into portfolio weights and hence, in the pipeline, this is **the most critical** step since it establishes a common portfolio contruction baseline for both Monte-Carlo and XGBoost implementations - there by - allowing the risk profiles (generated from each) to dictate the evaluation. For this analysis, we have considered **\"Maximizing Sharpe\"** as the portfolio construction rule. However, there are other alternatives such as:\n",
    "\n",
    " - **Mean Variance Optimization (Markowitz):**\n",
    "   Focuses on maximizing portfolio variance subject to a target return (or vice versa).\n",
    "   \n",
    "   Key characteristics:\n",
    "    - Requires selecting a target return $r$\n",
    "    - Highly sensitive to estimation error in expected returns\n",
    "    - Foundational but fragile in practice\n",
    "\n",
    "   Objective:\n",
    "$$\n",
    "min_{w}(w^T\\Sigma w)\n",
    "\\;\\text{subject to:}\\;\n",
    "w^T \\mu >= r;\\quad \\Sigma_i w_i = 1\n",
    "$$\n",
    "\n",
    " - **Maximize Sharpe Ratio (Tangency Portfolio) - Selected Rule:**\n",
    "   Focuses on maximizing portfolio risk-adjusted return by choosing weights that yeild the highest expected excess return per unit of total volatility (Sharpe Ratio).\n",
    "   \n",
    "   Key characteristics:\n",
    "    - Maximizes risk-adjusted return\n",
    "    - Produces a unique optimal portfolio without arbitrary targets\n",
    "\n",
    "   Objective:\n",
    "$$\n",
    "max_{w}(\\frac{(w^T\\mu - r_f)}{\\sqrt{w^T\\Sigma w}})\n",
    "\\;\\text{subject to:}\\;\n",
    "w_i >= 0;\\quad \\Sigma_i w_i = 1\n",
    "$$\n",
    "\n",
    " - **Minimum Variance Portfolio (MVP):**\n",
    "   Focuses on minimizing total portfolio variance without regard to expected returns, producing the least volatile - fully invested portfolio.\n",
    "   \n",
    "   Key characteristics:\n",
    "    - Ignores expected returns entirely\n",
    "    - Extremely stable\n",
    "\n",
    "   Objective:\n",
    "$$\n",
    "min_{w}(w^T\\Sigma w)\n",
    "\\;\\text{subject to:}\\;\n",
    "\\Sigma_i w_i = 1\n",
    "$$\n",
    "\n",
    " - **Risk Parity/ Equal Risk Contribution (ERC):**\n",
    "   Focuses on allocating weights so that each asset contributes equally to total portfolio risk, promoting balanced diversification across assets.\n",
    "   \n",
    "   Key characteristics:\n",
    "    - Uses only the covariance structure\n",
    "    - Popular in multi-asset institutional portfolios\n",
    "\n",
    "   Objective:\n",
    "$$\n",
    "RiskContribution_i = w_i\\frac{(\\Sigma w)_{i}}{\\sqrt{w^T\\Sigma w}}\\;\\text{equal for all}\\;{i}\n",
    "$$\n",
    "\n",
    " - **Minimum Diversification Ratio:**\n",
    "   Focuses on minimizing the ratio of weighted individual asset volatilities to overall portfolio volatility, thereby exploiting diversification benefits from imperfect correlations.\n",
    "   \n",
    "   Key characteristics:\n",
    "    - Maximizes diversification benefit\n",
    "    - Closely related to risk parity\n",
    "\n",
    "   Objective:\n",
    "$$\n",
    "max_{w}(\\frac{w^T\\sigma}{\\sqrt{w^T\\Sigma w}})\n",
    "\\quad \\text{where:}\\;\\sigma\\;\\text{is the vector of individual asset volatilities.}\n",
    "$$\n",
    "\n",
    " - **CVaR (Expected Shortfall) Minimization:**\n",
    "   Focuses on minimizing expected losses in the worst-case tail of the return distribution, focusing on downside risk rather than overall variance.\n",
    "   \n",
    "   Key characteristics:\n",
    "    - Focuses on downside tail risk\n",
    "    - Suitable for drawdown-sensitive investors\n",
    "\n",
    "   Objective:\n",
    "$$\n",
    "min_{w}CVaR_\\alpha(w)\n",
    "$$\n",
    "\n",
    " - **Utility-Based Optimization (CRRA/ CARA):**\n",
    "   Focuses on minimizing the investor’s expected utility of portfolio returns, explicitly trading off return and risk according to a specified risk-aversion preference..\n",
    "   \n",
    "   Key characteristics:\n",
    "    - Requires specifying investor risk aversion\n",
    "    - Difficult to compare across models due to subjective parameters\n",
    "\n",
    "   Objective:\n",
    "$$\n",
    "max_{w}\\mathbb{E}[U(r_p)]\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
