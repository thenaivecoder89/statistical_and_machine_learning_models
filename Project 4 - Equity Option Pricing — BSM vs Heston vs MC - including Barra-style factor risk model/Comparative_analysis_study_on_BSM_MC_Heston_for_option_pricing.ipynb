{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52393956",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A Comparative Benchmarking Study of Monte-Carlo, Black-Scholes-Merton and Heston Models for Equity Option Pricing\"\n",
    "format:\n",
    "  pdf:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    geometry:\n",
    "      - margin=15mm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb91126",
   "metadata": {},
   "source": [
    "## Section 1: Introduction and Summary\n",
    "\n",
    "**1.1. Project Overview and Objective**\n",
    "\n",
    "Equity option pricing remains a foundational problem in quantitative finance, with widespread practical applications spanning trading, risk management, and regulatory capital assessment. Despite decades of academic development, practitioners continue to rely on a small set of canonical models, each embodying distinct assumptions regarding asset dynamics, volatility behavior, and distributional properties of returns.\n",
    "\n",
    "The objective of this study is to systematically benchmark three widely used equity option pricing frameworks — Monte-Carlo simulation, Black-Scholes-Merton (BSM), and the Heston stochastic volatility model — with the explicit aim of documenting their relative strengths, weaknesses, and practical limitations under realistic data and governance constraints. The benchmarking exercise is conducted using two parallel datasets: a proxy-based dataset designed for model development and regulatory defensibility, and an ETF-based dataset used for external price comparison and realism checks.\n",
    "\n",
    "Rather than proposing a novel pricing model, this research focuses on comparative robustness, distributional realism, and out-of-sample behavior, reflecting the considerations faced by risk and model validation teams in institutional settings.\n",
    "\n",
    "\n",
    "**1.2. Overview of the Models Considered**\n",
    "\n",
    "The three models examined in this study span a spectrum of theoretical assumptions and practical complexity:\n",
    "\n",
    "- Black-Scholes-Merton (BSM) represents the classical closed-form solution for European option pricing, assuming log-normally distributed asset prices, constant volatility, frictionless markets, and continuous hedging. Despite well-documented empirical violations of these assumptions, BSM remains an industry benchmark due to its interpretability, analytical tractability, and historical adoption.\n",
    "\n",
    "- The Heston stochastic volatility model extends the BSM framework by introducing a mean-reverting stochastic process for volatility, thereby capturing volatility clustering and leverage effects observed in real markets. While more flexible than BSM, the Heston model introduces calibration complexity, numerical stability considerations, and additional parameter uncertainty.\n",
    "\n",
    "- Monte-Carlo simulation, implemented in this study using a bootstrap-based, non-parametric approach, avoids strong parametric assumptions about return distributions. Instead, it relies on empirically observed return dynamics to generate future price paths, allowing for skewness, excess kurtosis, and regime-specific behavior to be preserved in simulated outcomes.\n",
    "\n",
    "Together, these models enable a structured comparison between analytical tractability, stochastic realism, and distribution-free empirical modeling.\n",
    "\n",
    "\n",
    "**1.3. Data Architecture and Dual-ETL Design**\n",
    "\n",
    "A key design feature of this project is the construction of two distinct ETL pipelines, motivated by data availability constraints and regulatory considerations.\n",
    "\n",
    "- The proxy-based dataset is constructed using liquid, publicly available equity and index data to create synthetic underlying assets suitable for model development, stress testing, and out-of-sample evaluation. This pipeline emphasizes transparency, reproducibility, long historical coverage, and suitability for statistical testing and regulatory defense. The proxy dataset serves as the primary foundation for model calibration, simulation, and risk evaluation.\n",
    "\n",
    "- The ETF-based dataset sources option-relevant price information derived from exchange-traded funds tracking the same or closely related underlying assets. This pipeline is used exclusively for external price comparison, visual benchmarking, and plausibility checks of model-implied option prices. Due to limitations associated with free and publicly accessible option datasets, the ETF-based pipeline is not used for model calibration or statistical inference.\n",
    "\n",
    "\n",
    "**1.4. Statistical Diagnostics and Distributional Testing**\n",
    "\n",
    "Prior to model implementation, extensive statistical diagnostics are applied to the proxy-based dataset to assess the validity of common modeling assumptions. These diagnostics include:\n",
    "\n",
    "- Tests for normality and log-normality of returns\n",
    "- Evaluation of skewness and excess kurtosis\n",
    "- Analysis of volatility clustering and regime dependence\n",
    "\n",
    "Empirical evidence of non-Gaussian return behavior, fat tails, and asymmetry motivates the inclusion of a bootstrap-based Monte-Carlo framework and informs the interpretation of results obtained from parametric models such as BSM and Heston.\n",
    "\n",
    "\n",
    "**1.5. Out-of-Sample Evaluation Framework**\n",
    "\n",
    "Model evaluation is conducted using a clearly defined out-of-sample (OOS) framework, designed to assess robustness rather than in-sample fit. OOS parameters are selected to capture multiple dimensions of model performance, including:\n",
    "\n",
    "- Pricing stability across market regimes\n",
    "- Tail risk behavior and downside sensitivity\n",
    "- Risk decomposition and factor exposure consistency\n",
    "\n",
    "The evaluation framework emphasizes comparative behavior under identical inputs, ensuring that differences in outcomes can be attributed to model structure rather than data artifacts.\n",
    "\n",
    "\n",
    "**1.6. Implementation Methodology (Summary)**\n",
    "\n",
    "The overall implementation methodology follows a structured and reproducible sequence:\n",
    "\n",
    "- Construction of proxy-based and ETF-based ETL pipelines\n",
    "- Statistical diagnostics and assumption testing on proxy data\n",
    "- Independent implementation of Monte-Carlo, BSM, and Heston models\n",
    "- Model calibration and numerical validation, where applicable\n",
    "- ETF-based benchmarking for external pricing realism\n",
    "- Proxy-based out-of-sample risk and stability evaluation\n",
    "- Comparative analysis across all models and evaluation metrics\n",
    "\n",
    "\n",
    "**1.7. Summary of Findings**\n",
    "\n",
    "**TO BE FILLED IN THE END**\n",
    "\n",
    "\n",
    "**1.8. Compliance with SR 11-7 Model Risk and Governance Framework**\n",
    "\n",
    "**TO BE FILLED IN THE END**\n",
    "\n",
    "\n",
    "**Disclaimer**\n",
    "\n",
    "This document is intended solely for academic, educational, and research purposes. The models, methodologies, data sources, and results presented herein are illustrative in nature and are not intended to constitute investment advice, trading recommendations, or financial forecasts. Any references to market instruments, prices, or returns are for analytical demonstration only. The analyses rely on publicly available data and simplifying assumptions, and may not fully capture real-world market frictions, liquidity constraints, transaction costs, or institutional trading considerations. This document does not intend to provide representations regarding the suitability of any model or result for live trading, risk management, or regulatory capital determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d15f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os, time, requests\n",
    "from dotenv import load_dotenv\n",
    "from contextlib import contextmanager\n",
    "from IPython.display import display\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, yfinance as yf\n",
    "\n",
    "# Initializing environment\n",
    "load_dotenv()\n",
    "zero_coupon = os.getenv('zero_coupon')\n",
    "proxy_master = os.getenv('proxy_master')\n",
    "etf_master = os.getenv('etf_master')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Defining time context block\n",
    "@contextmanager\n",
    "def time_block(label: str='Block'):\n",
    "    start_time = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        end_time = time.perf_counter()\n",
    "        runtime = end_time - start_time\n",
    "        print(f'{label} execution time: {runtime/60:.2f} minutes.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e5609",
   "metadata": {},
   "source": [
    "## Section 2: Implementation of ETL Pipelines\n",
    "\n",
    "**1.1. Overview**\n",
    "\n",
    "This project adopts a dual data-architecture strategy to balance **economic correctness**, **empirical validation**, and **data accessibility constraints**:\n",
    "\n",
    "- **Proxy-based architecture**:\n",
    "  This architecture constructs a synthetic yet economically grounded state-space of contingent claims, designed to support model development, assumption testing, and controlled out-of-sample (OOS) evaluation. The approach deliberately decouples contract specification from market-quoted option chains by defining options across fixed grids of moneyness, maturity, and payoff type, while anchoring all valuations to observed underlying prices, realized volatility proxies, risk-free term structures, and dividend yield assumptions. By holding the contract space and input state variables constant, the framework enables clean comparative benchmarking of pricing models, ensuring that observed differences in valuation and risk metrics are attributable to model dynamics rather than data availability, liquidity effects, or market microstructure noise. This proxy-based design emphasizes transparency, economic interpretability, numerical stability, and robustness under model governance and regulatory scrutiny, consistent with principles outlined in formal model-risk management frameworks.\n",
    "\n",
    "- **ETF-based architecture**: \n",
    "  This architecture leverages exchange-traded fund (ETF) option chains as market-anchored representations of contingent claims, designed to support empirical validation, calibration realism, and external consistency checks against observed option prices. The approach preserves the contract specifications embedded in traded ETF options, including listed strikes, standardized maturities, and observed call/put structures, while anchoring valuations to market-implied prices, bid–ask dynamics, and prevailing liquidity conditions. As a result, the ETF-based framework reflects the joint influence of investor demand, hedging activity, volatility risk premia, and market microstructure effects inherent in real-world options markets. By operating directly on quoted option chains, the architecture enables assessment of absolute pricing accuracy, calibration stability, and model fit to observed market prices, complementing the controlled benchmarking enabled by the proxy-based framework. Differences between model-implied and market-observed prices can therefore be interpreted in the context of model assumptions, volatility dynamics, and unmodeled risk premia, rather than purely synthetic contract design. This ETF-based design emphasizes market realism, empirical validity, and external benchmark alignment, providing a necessary counterbalance to synthetic architectures and supporting robust model validation, sensitivity analysis, and governance-oriented performance assessment when used in conjunction with controlled proxy-based evaluations.\n",
    "\n",
    "- **Why a dual-architecture setup**:\n",
    "  The dual-architecture design addresses the absence of licensed market data feeds (e.g., Bloomberg, Refinitiv, Eurex) while maintaining methodological rigor consistent with institutional model development practices. Early stages of model development—including assumption testing, sensitivity analysis, and numerical stability checks—require controlled experimentation, isolation of effects, and a repeatable state-space. These requirements are satisfied by the proxy-based architecture, but cannot be reliably met using a purely ETF-based framework due to changing contract availability, liquidity effects, and embedded market risk premia. In contrast, later stages of model calibration and out-of-sample (OOS) validation benefit from direct exposure to market-quoted option prices, where an ETF-based architecture provides a realistic representation of observed pricing dynamics that a proxy-based framework cannot replicate. This separation is consistent with formal model-risk management guidance. In particular, SR 11-7 states that *“model validation should employ a combination of theoretical evaluation, controlled testing, and benchmarking against observed outcomes.”* The dual-architecture setup operationalizes this principle by explicitly separating controlled model diagnostics from market-anchored validation, ensuring both interpretability and empirical relevance.\n",
    "\n",
    ">**Note**: To deliberately eliminate confounding effects - attributable to cross-asset differences arising from leveraging different indices under proxy and ETF architecture - both architectures use the same EUR-denominated underlying (EXSA.DE - iShares Core EURO STOXX 50 UCITS, ETF, EUR, Xetra). The proxy-based architecture generates synthetic option contracts and model-implied prices to enable controlled comparison of model dynamics under consistent inputs. The ETF-based architecture uses observed option quotes on the same underlying to provide empirical outcomes analysis, acknowledging market microstructure noise, discrete dividend effects, and liquidity filters.\n",
    "\n",
    "\n",
    "**1.2. ETL Pipeline**\n",
    "\n",
    "In alignment with the SR 11-7 Model Risk Management framework, the project implements the following standardized ETL pipeline for both data architectures:\n",
    "\n",
    "- **Step 1: Data Acquisition:** Sourcing raw market observables from documented, reproducible open-source channels.\n",
    "\n",
    "- **Step 2: Data Ingestion:** Structured loading into analysis-ready data containers with version control and timestamping.\n",
    "\n",
    "- **Step 3: Data Cleansing:** Handling missing values, calendar alignment, corporate action adjustments, and data sanity checks.\n",
    "\n",
    "- **Step 4: Feature Engineering (as required):** Construction of derived quantities such as log returns, realized volatility measures, and term-structure interpolations.\n",
    "\n",
    "- **Step 5: Rendering Model-Ready Datasets:** Final transformation into inputs compatible with Black–Scholes–Merton, Heston, and Monte Carlo pricing frameworks.\n",
    "\n",
    "This pipeline separation ensures traceability, auditability, and reproducibility—key SR 11-7 requirements.\n",
    "\n",
    "\n",
    "**1.3. Data Acquisition Sources and Rationale — Proxy-Based Architecture**\n",
    "\n",
    "Under the proxy-based architecture, the following data components are sourced to construct a **theoretically consistent index-level pricing environment**. These inputs are used for model calibration, assumption testing, and controlled comparative analysis rather than direct replication of observed option prices.\n",
    "\n",
    "Main Area 1: Underlying Index Prices - *Used to compute log returns, realized volatility, and to define the underlying state variable.*\n",
    "\n",
    "- Daily closing prices - *Source: Yahoo Finance – EXSA.DE (iShares Core EURO STOXX 50 UCITS ETF, EUR, Xetra)*  \n",
    "- Trading calendar and date alignment - *Derived from ETF price time series*\n",
    "\n",
    "Main Area 2: Realized Volatility Proxy - *Used to characterize historical volatility dynamics and provide data-driven inputs and initial conditions for stochastic volatility models.*\n",
    "\n",
    "- Rolling realized volatility (21 days) - *Engineered from log returns derived in Main Area 1*  \n",
    "- Rolling realized volatility (63 days) - *Engineered from log returns derived in Main Area 1*\n",
    "- EWMA realized volatility - *Computed from historical log returns using exponentially decaying weights*\n",
    "\n",
    "> **Note:** Full implied volatility surfaces are not available via open-source channels for European indices. Volatility indices are therefore engineered for this project using realized returns data.\n",
    "\n",
    "Main Area 3: Risk-Free Rate Term Structure - *Used for discounting option payoffs and defining the risk-neutral drift.*\n",
    "\n",
    "- Zero-coupon yields by maturity - *Source: European Central Bank – Data Portal*\n",
    "\n",
    "> **Note:** Annualized continuously compounded zero-coupon yields - published on an ACT/365 basis - across multiple maturities observed daily. Retrieval mode: JSON dumps from public URLs published by ECB. \n",
    "\n",
    "Main Area 4: Dividend Yield - *Used to correct forward price dynamics and maintain put–call parity consistency.*\n",
    "\n",
    "- Index-level dividend yield (spot or trailing) - *Source: Yahoo Finance - EXSA.DE – index `dividendYield` field*\n",
    "\n",
    "> **Note:** The dividend yield is treated as constant over time, consistent with standard equity option pricing practice.\n",
    "\n",
    "Main Area 5: Option Chain Construction and Pricing - *Used for inter-model benchmarking, internal consistency checks, and analysis of model-implied smile and skew behavior.*\n",
    "\n",
    "- Option type (Call / Put) - *Constructed using simple classification*\n",
    "- Strike price grid (K) - *Constructed using volatility-scaled spot log-moneyness. $K = S_t * e^k$ where $k = L * \\sigma_a * \\sqrt(T)$*\n",
    "- Time to maturity (T) - *Constructed - as per zero-coupon maturity dates*\n",
    "\n",
    ">**Note:**\n",
    ">To preserve economic relevance and numerical stability, the log-moneyness domain is capped between -0.7 and 0.7 while allowing volatility regimes to retain regime shocks and affect pricing within the admissible contract space. Without capping our k-values amplify mathematically (not economically) since $k \\propto \\sigma * \\sqrt{T} * L$ hence, during high time to maturity and volatility grids (e.g., 2008 + 10Y) the k-values get mathematically amplified - which lead to Far OTM and Deep ITM strike prices - rendering price ~0, greek ~0 and all 3 models being numerically indistinguishable from one another. Hence, to avoid this mathematical noise, capping has been enforced.\n",
    "\n",
    "> **Important Clarification:**  \n",
    "> Under the proxy-based architecture, **no observed market option prices are sourced**. Option prices are fully **model-implied**, enabling controlled comparison of pricing dynamics across Black–Scholes–Merton, Heston, and Monte Carlo frameworks without contamination from microstructure noise or liquidity effects.\n",
    "\n",
    "\n",
    "**1.4. Data Acquisition Sources and Rationale — ETF-Based Architecture**\n",
    "\n",
    "The ETF-based architecture is introduced to provide **empirical pricing benchmarks** using observable option markets on highly liquid exchange-traded funds. While ETFs introduce tracking error and structural noise, their option chains offer the only feasible open-source alternative for observed option prices. Additionally, under the ETF-based architecture, the ETF itself is treated as the underlying tradable asset; index replication is not assumed.\n",
    "\n",
    "The data points mirror those used in the proxy-based architecture to ensure methodological consistency; however, **data sources differ materially**.\n",
    "\n",
    "Main Area 1: Underlying ETF Prices - *Used as the tradable underlying for observed option contracts.*\n",
    "\n",
    "- Daily closing and adjusted prices - *Source: Yahoo Finance – EXSA.DE (iShares Core EURO STOXX 50 UCITS ETF, EUR, Xetra)*  \n",
    "- Trading calendar - *Derived from ETF price series*\n",
    "\n",
    "Main Area 2: Implied Volatility (Observed) - *Derived from observable option market prices for liquid ETFs and used for empirical validation.*\n",
    "\n",
    "- Option-implied volatility (by strike and maturity) - *Extracted from Yahoo Finance ETF option chains and cleaned for liquidity and data quality*\n",
    "\n",
    "Main Area 3: Risk-Free Rate Term Structure - *Used consistently across both architectures to maintain comparability.*\n",
    "\n",
    "- Zero-coupon yields by maturity - *Source: ECB SDW API for EUR-denominated ETFs; FRED for USD-denominated ETFs*\n",
    "\n",
    "Main Area 4: Dividend Yield - *Embedded in ETF price dynamics and option pricing.*\n",
    "\n",
    "- ETF dividend yield - *Source: Yahoo Finance – ETF `dividendYield` field*\n",
    "\n",
    "Main Area 5: Observed Option Chain Data - *Used for direct model-to-market pricing comparison and OOS evaluation.*\n",
    "\n",
    "- Option type (Call / Put) - *Source: Yahoo Finance option chains*\n",
    "- Strike price (K) - *Source: Yahoo Finance option chains*\n",
    "- Expiration date and time to maturity (T) - *Source: Yahoo Finance option chains*\n",
    "- Market option prices (mid, bid–ask) - *Source: Yahoo Finance option chains*\n",
    "- Volume and open interest (where available) - *Source: Yahoo Finance option chains*\n",
    "\n",
    "\n",
    "**Governance Note (SR 11-7 Alignment)**\n",
    "\n",
    "The separation of proxy-based and ETF-based architectures ensures:\n",
    "\n",
    "- Explicit documentation of data limitations and assumptions\n",
    "- Clear distinction between **model development** and **empirical validation**\n",
    "- Avoidance of overfitting or misrepresentation of model accuracy\n",
    "- Traceability of all inputs to reproducible sources\n",
    "\n",
    "This design aligns with SR 11-7 expectations regarding model transparency, validation independence, and controlled use of approximations.\n",
    "\n",
    "\n",
    "**Section Output:**\n",
    "At the end of the ETL process, the project produces separate market-state datasets and option-contract datasets for both proxy-based and ETF-based architectures. The proxy datasets support controlled model development and assumption testing, while the ETF datasets enable empirical validation against observed option prices. Model prices generated under the ETF architecture are compared against market option prices—not against proxy-generated prices—to ensure economically meaningful validation and SR 11-7-compliant separation of development and benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd05bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_zero_coupon_data():\n",
    "    url = [\n",
    "        'https://data-api.ecb.europa.eu/service/data/YC/B.U2.EUR.4F.G_N_A.SV_C_YM.SR_3M?startPeriod=2008-04-04&endPeriod=2026-01-30&format=jsondata',\n",
    "        'https://data-api.ecb.europa.eu/service/data/YC/B.U2.EUR.4F.G_N_A.SV_C_YM.SR_6M?startPeriod=2008-04-04&endPeriod=2026-01-30&format=jsondata',\n",
    "        'https://data-api.ecb.europa.eu/service/data/YC/B.U2.EUR.4F.G_N_A.SV_C_YM.SR_1Y?startPeriod=2008-04-04&endPeriod=2026-01-30&format=jsondata',\n",
    "        'https://data-api.ecb.europa.eu/service/data/YC/B.U2.EUR.4F.G_N_A.SV_C_YM.SR_2Y?startPeriod=2008-04-04&endPeriod=2026-01-30&format=jsondata',\n",
    "        'https://data-api.ecb.europa.eu/service/data/YC/B.U2.EUR.4F.G_N_A.SV_C_YM.SR_5Y?startPeriod=2008-04-04&endPeriod=2026-01-30&format=jsondata',\n",
    "        'https://data-api.ecb.europa.eu/service/data/YC/B.U2.EUR.4F.G_N_A.SV_C_YM.SR_10Y?startPeriod=2008-04-04&endPeriod=2026-01-30&format=jsondata'\n",
    "    ]\n",
    "    data_dump_yield = []\n",
    "    data_dump_dates = []\n",
    "    data_dump_ids = []\n",
    "    df_zero_coupon = pd.DataFrame()\n",
    "    for i in url:\n",
    "        req = requests.get(i, timeout=60)\n",
    "        req.raise_for_status()\n",
    "        json_dump = req.json()\n",
    "        yield_data = json_dump['dataSets'][0]['series']['0:0:0:0:0:0:0']['observations']\n",
    "        for i in range(len(yield_data)):\n",
    "            data_dump_yield.append(yield_data[str(i)][0])\n",
    "            id_data = json_dump['structure']['dimensions']['series'][6]['values'][0]['id']\n",
    "            data_dump_ids.append(id_data)\n",
    "\n",
    "        time_data = json_dump['structure']['dimensions']['observation'][0]['values']\n",
    "        for i in range(len(time_data)):\n",
    "            data_dump_dates.append(time_data[i]['name'])\n",
    "\n",
    "    \n",
    "    df_zero_coupon['yields'] = data_dump_yield\n",
    "    df_zero_coupon['yields'] = df_zero_coupon['yields'] / 100\n",
    "    df_zero_coupon['Date'] = data_dump_dates\n",
    "    df_zero_coupon['ID'] = data_dump_ids\n",
    "    df_zero_coupon['Date'] = pd.to_datetime(df_zero_coupon['Date'])\n",
    "    df_zero_coupon = (\n",
    "        df_zero_coupon\n",
    "        .pivot(index='Date', columns='ID', values='yields')\n",
    "        .reset_index()\n",
    "    )\n",
    "    df_zero_coupon = df_zero_coupon.set_index('Date')\n",
    "    df_zero_coupon.to_csv(zero_coupon)\n",
    "    return df_zero_coupon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f91561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proxy-based architecture implementation\n",
    "def proxy_architecture_data_generation():\n",
    "    # 1. Index closing prices\n",
    "    df_proxy_index = yf.download('EXSA.DE', start='2008-01-01', end='2026-02-01', progress=False)\n",
    "    df_proxy_index = (\n",
    "        df_proxy_index\n",
    "        .xs('Close', level='Price', axis=1)\n",
    "        .rename(columns={'EXSA.DE': 'index_closing_price'})\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    # 3. Zero-coupon yield\n",
    "    with time_block('Zero Coupon Data Load'):\n",
    "        df_zero_coupon = pull_zero_coupon_data()\n",
    "\n",
    "    df_proxy_index = df_proxy_index.join(\n",
    "        df_zero_coupon,\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # 4. Dividend-yield\n",
    "    exsa = yf.Ticker('EXSA.DE')\n",
    "    div = exsa.info.get('dividendYield') / 100\n",
    "\n",
    "\n",
    "    # Performing data consolidation, engineering and sanity checks\n",
    "    # Data engineering\n",
    "    df_proxy_index['index_closing_simple_ret'] = df_proxy_index['index_closing_price'].pct_change()\n",
    "    df_proxy_index['index_closing_log_ret'] = np.log(1 + df_proxy_index['index_closing_simple_ret'])\n",
    "    # 2. Volatility proxies (calculated in this section to avoid data conflicts)\n",
    "    lam = 0.94\n",
    "    df_proxy_index['daily_rol_vol_21D'] = df_proxy_index['index_closing_log_ret'].rolling(21).std(ddof=1)\n",
    "    df_proxy_index['daily_rol_vol_63D'] = df_proxy_index['index_closing_log_ret'].rolling(63).std(ddof=1)\n",
    "    df_proxy_index['daily_ewma_vol'] = np.sqrt(df_proxy_index['index_closing_log_ret'].pow(2).ewm(alpha=1-lam, adjust=False).mean()) # set adjust = False to compute volatility using the classical EWMA vol model. True will use the statistical definition.\n",
    "    df_proxy_index['dividend_yield'] = div\n",
    "    df_proxy_index = df_proxy_index.dropna(how='any')\n",
    "    # 5. Option chain construction (implemented here to preserve data structure)\n",
    "    # Maturity dates construction\n",
    "    T_in_years = [0.25, 0.50, 1.00, 2.00, 5.00, 10.00]\n",
    "    df_proxy_index = (\n",
    "        df_proxy_index\n",
    "        .loc[df_proxy_index.index.repeat(len(T_in_years))]\n",
    "        .assign(T_in_years=T_in_years * len(df_proxy_index))\n",
    "        .set_index('T_in_years', append=True)\n",
    "        .sort_index()\n",
    "    )\n",
    "    # Strike price construction\n",
    "    L_grid = np.array([-2.5,-2.0,-1.5,-1.0,-0.5,0,0.5,1.0,1.5,2.0,2.5])\n",
    "    n = len(df_proxy_index)\n",
    "    df_proxy_index = (\n",
    "        df_proxy_index\n",
    "        .loc[df_proxy_index.index.repeat(len(L_grid))]\n",
    "    )\n",
    "    df_proxy_index['L'] = np.tile(L_grid, n)\n",
    "    df_proxy_index['annualized_ewma_vol'] = df_proxy_index['daily_ewma_vol'].to_numpy() * np.sqrt(252)\n",
    "    k_raw = df_proxy_index['L'] * df_proxy_index['annualized_ewma_vol'] * np.sqrt(df_proxy_index.index.get_level_values('T_in_years'))\n",
    "    df_proxy_index['k'] = np.clip(k_raw, -0.7, 0.7)\n",
    "    df_proxy_index['K'] = df_proxy_index['index_closing_price'] * np.exp(df_proxy_index['k'])\n",
    "    # Call/ Put classification construction\n",
    "    c_p_classification = ['C', 'P']\n",
    "    df_proxy_index = (\n",
    "        df_proxy_index\n",
    "        .loc[df_proxy_index.index.repeat(2)]\n",
    "    )\n",
    "    df_proxy_index['call_put_classification'] = np.tile(c_p_classification, len(df_proxy_index)//2)\n",
    "    df_class_check = (\n",
    "        df_proxy_index.groupby([\n",
    "            pd.Grouper(level='Date'),\n",
    "            pd.Grouper(level='T_in_years'),\n",
    "            'K'\n",
    "        ])['call_put_classification']\n",
    "        .nunique()\n",
    "    )\n",
    "    mismatch = df_class_check[df_class_check < 2]\n",
    "    if len(mismatch) > 0:\n",
    "        print(f'{len(mismatch)} records with incorrect classification structure found.')\n",
    "    else:\n",
    "        print('Classification structure complete.')\n",
    "\n",
    "    # Data sanity checks\n",
    "    assert df_proxy_index.index.is_monotonic_increasing\n",
    "    assert (df_proxy_index['index_closing_simple_ret'] > -1).all()\n",
    "\n",
    "    # Dropping duplicates and creating final master\n",
    "    df_proxy_index_master = (\n",
    "        df_proxy_index\n",
    "        .reset_index()\n",
    "        .drop_duplicates(subset=['Date', 'T_in_years', 'L', 'call_put_classification'], keep='first')\n",
    "        .sort_values(['Date', 'T_in_years', 'L', 'call_put_classification'])\n",
    "        .set_index(['Date', 'T_in_years'])\n",
    "    )\n",
    "    \n",
    "    # Configuring data display template\n",
    "    df_proxy_index_master.style.set_table_styles(\n",
    "        [\n",
    "            {\n",
    "                'selector': 'th',\n",
    "                'props': [('text-align','center')]\n",
    "            },\n",
    "            {\n",
    "                'selector': 'td',\n",
    "                'props': [('text-align', 'right')]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Diagnostic print statements\n",
    "    print('Data sourced for index prices:')\n",
    "    display(df_proxy_index_master.head(44))\n",
    "    print(f'Total number of records in master proxy data: {len(df_proxy_index_master)}')\n",
    "    \n",
    "    # Uploading data to pickle file\n",
    "    df_proxy_index_master.to_pickle(proxy_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88892500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling data generation programs\n",
    "# with time_block('Proxy data generation block'):\n",
    "#     proxy_architecture_data_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccb0cddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker      index_closing_price\n",
      "Date                           \n",
      "2008-01-02              24.6418\n",
      "2008-01-03              24.5673\n",
      "2008-01-04              24.0930\n",
      "2008-01-07              24.0456\n",
      "2008-01-08              24.1879\n"
     ]
    }
   ],
   "source": [
    "# ETF-based architecture implementation\n",
    "# 1. Index closing prices\n",
    "df_etf_index = yf.download('EXSA.DE', start='2008-01-01', end='2026-02-01', progress=False)\n",
    "df_etf_index = (\n",
    "    df_etf_index\n",
    "    .xs('Close', level='Price', axis=1)\n",
    "    .rename(columns={'EXSA.DE': 'index_closing_price'})\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "print(df_etf_index.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
